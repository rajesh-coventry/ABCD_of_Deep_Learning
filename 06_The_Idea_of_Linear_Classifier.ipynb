{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08d3d282",
   "metadata": {},
   "source": [
    "# **The Idea of Linear Classifier:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499ee268",
   "metadata": {},
   "source": [
    "### **1. What is a classifier?**\n",
    "\n",
    "At the most basic level, a **`classifier`** is a function that:\n",
    "\n",
    "> **Assigns an input to one of several categories (classes).**\n",
    "\n",
    "**Examples:**   \n",
    "   * Email → spam / not spam\n",
    "   * Image → cat / dog\n",
    "   * Sensor data → normal / abnormal\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "> $f(x) \\rightarrow \\text{class label}$ \n",
    "\n",
    "### **2. What makes a classifier “linear”?**\n",
    "\n",
    "A classifier is called **linear** if its decision is based on a **linear function of the input**.\n",
    "\n",
    "That linear function is:\n",
    "\n",
    "> $z = w^\\top x + b$ \n",
    "\n",
    "Where:   \n",
    "   * ($x \\in \\mathbb{R}^n$) is the input vector\n",
    "   * ($w \\in \\mathbb{R}^n$) is the weight vector\n",
    "   * ($b$) is a bias (threshold)\n",
    "\n",
    "The classifier then applies a **decision rule**:\n",
    "\n",
    "> $\\text{class} =\n",
    "\\begin{cases}\n",
    "+1 & \\text{if } z \\ge 0 \\\\\n",
    "-1 & \\text{if } z < 0\n",
    "\\end{cases}$ \n",
    "\n",
    "This is the **essence of a linear classifier**.\n",
    "\n",
    "### **3. Geometric meaning (core intuition):**\n",
    "\n",
    "The equation:\n",
    "\n",
    "> $w^\\top x + b = 0$ \n",
    "\n",
    "defines a **hyperplane** in input space.\n",
    "\n",
    "* In 2D → a line\n",
    "* In 3D → a plane\n",
    "* In higher dimensions → a hyperplane\n",
    "\n",
    "This hyperplane:\n",
    "\n",
    "* **Separates space into two halves**\n",
    "* Each half corresponds to a class\n",
    "\n",
    "So a linear classifier:\n",
    "\n",
    "> **Decides class membership by which side of a hyperplane a point lies on.**\n",
    "\n",
    "### **4. Why linear classifiers matter historically:**\n",
    "\n",
    "Before learning rules existed, people needed to answer:\n",
    "\n",
    "> *What kind of decision can a single neuron make?*\n",
    "\n",
    "The answer:   \n",
    "   * A neuron computes a weighted sum\n",
    "   * Applies a threshold\n",
    "   * Outputs a class\n",
    "\n",
    "That is *exactly* a linear classifier.\n",
    "\n",
    "So historically:    \n",
    "   * **MCP neuron = fixed linear classifier**\n",
    "   * **Perceptron = learned linear classifier**\n",
    "\n",
    "### **5. How Linear Classifiers fit into Our Learning Pipeline?**\n",
    "\n",
    "Let’s place them precisely.\n",
    "\n",
    "**MCP neuron:**\n",
    "\n",
    "> $y = H(w^\\top x - \\theta)$ \n",
    "\n",
    "* Fixed weights\n",
    "* Hard-coded decision boundary\n",
    "* Linear classifier with no learning\n",
    "\n",
    "**Hebb / Oja:**\n",
    "\n",
    "* Learn weights from data\n",
    "* No target labels\n",
    "* Discover dominant directions\n",
    "* Still linear projections\n",
    "\n",
    "They shape the hyperplane but **not toward a classification goal**.\n",
    "\n",
    "**Delta Rule:**\n",
    "\n",
    "> $\\Delta w = \\eta (t - y)x$\n",
    "\n",
    "* Learns weights using labeled data\n",
    "* Minimizes squared error\n",
    "* Learns the **best linear fit**\n",
    "\n",
    "But output is still continuous.\n",
    "\n",
    "**Perceptron:**\n",
    "\n",
    "> $y = \\text{sign}(w^\\top x + b)$ \n",
    "\n",
    "* Converts linear output into class labels\n",
    "* Updates only on misclassification\n",
    "* Fully realizes the **linear classifier concept**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db742fc",
   "metadata": {},
   "source": [
    "### **6. Linear Separability (critical concept):**\n",
    "\n",
    "A dataset is **`linearly separable`** if:\n",
    "\n",
    "> There exists at least one hyperplane that perfectly separates the classes.\n",
    "\n",
    "Visually:   \n",
    "   * OR → linearly separable\n",
    "   * AND → linearly separable\n",
    "   * XOR → **not linearly separable**\n",
    "\n",
    "This explains:   \n",
    "   * Why XOR failed for MCP\n",
    "   * Why Hebb and Delta couldn’t fix it\n",
    "   * Why multilayer networks were needed\n",
    "\n",
    "### **7. Mathematical Perspective:**\n",
    "\n",
    "Linear classifier defines:\n",
    "\n",
    "> $f(x) = \\text{sign}(w^\\top x + b)$ \n",
    "\n",
    "Learning means: Find $(w, b)$ such that\n",
    "\n",
    "> $y_i (w^\\top x_i + b) > 0\n",
    "\\quad \\forall i$\n",
    "\n",
    "This is a **`geometric constraint problem`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420576e6",
   "metadata": {},
   "source": [
    "### **8. Optimization view:**\n",
    "\n",
    "Different learning rules correspond to different optimization goals:\n",
    "\n",
    "| Rule       | Objective                      |\n",
    "| ---------- | ------------------------------ |\n",
    "| Hebb       | Correlation maximization       |\n",
    "| Oja        | Variance maximization          |\n",
    "| Delta      | Squared error minimization     |\n",
    "| Perceptron | Misclassification minimization |\n",
    "\n",
    "But all operate within the **same linear model**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954626e8",
   "metadata": {},
   "source": [
    "### **9. Why Linear Classifiers are Still Important Today?**\n",
    "\n",
    "Even modern deep networks:   \n",
    "   * End with a linear classifier\n",
    "   * Use learned representations + linear separation\n",
    "\n",
    "Examples:   \n",
    "   * Softmax layer\n",
    "   * Logistic regression head\n",
    "   * SVMs\n",
    "\n",
    "Linear classifiers are **not obsolete** — they are foundational.\n",
    "\n",
    "### **10. Intuitive summary (mental model):**\n",
    "\n",
    "Think of learning as:   \n",
    "   * Rotating a ruler (hyperplane)\n",
    "   * Sliding it with bias\n",
    "   * Until it separates points as well as possible\n",
    "\n",
    "That ruler is the **linear classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87dafa1",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd023c7a",
   "metadata": {},
   "source": [
    "> **A linear classifier assigns classes by separating input space with a hyperplane defined by a weighted sum and bias, and all early neuron models and learning rules are fundamentally attempts to learn such a separating hyperplane.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
