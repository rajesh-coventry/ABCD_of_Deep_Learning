{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2481fdb8",
   "metadata": {},
   "source": [
    "# **Forward Pass of a Perceptron: PyTorch Style: One Sample Input at a Time (Batch Size = $1$):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1146f4ed",
   "metadata": {},
   "source": [
    "**`Forward propagation`** (often called the `\"forward pass\"`) is the process where a single perceptron, which is the simplest form of a neural network, takes an input and calculates its final output or prediction. It is the initial, predictive step before any learning or weight adjustment occurs.\n",
    "\n",
    "The entire thing that happens during this process can be broken down into two main steps: the **`Weighted Sum`** calculation and the application of the **`Activation Function`**.\n",
    "\n",
    "#### **1. Weighted Sum Calculation (Pre-activation):**\n",
    "\n",
    "The perceptron receives one or more **`input values`** (e.g., $x_1, x_2, x_3, \\dots$). Each of these inputs is connected to the perceptron's core processing unit by a corresponding **`weight`** (e.g., $w_1, w_2, w_3, \\dots$).\n",
    "\n",
    "* **Multiplication:** The perceptron first multiplies each input value by its respective weight. This step assigns an **`importance`** to each input, as determined by the current values of the weights.\n",
    "\n",
    "* **`Summation`:** All these weighted input values are then added together.\n",
    "\n",
    "* **`Bias Addition`:** A single value called the **`bias` ($b$)** is added to this sum. The bias acts like an adjustable threshold, allowing the perceptron to shift its decision boundary without needing any input to be non-zero.\n",
    "\n",
    "The result of this calculation is a single number, often referred to as the **`pre-activation`** or the **`weighted sum`**. Mathematically, this internal sum is represented as $z = (w_1x_1 + w_2x_2 + w_3x_3 + \\dots) + b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dfe417",
   "metadata": {},
   "source": [
    "#### **2. Activation Function Application:**\n",
    "\n",
    "The weighted sum ($z$) calculated in the first step is then passed through an **`activation function`**.\n",
    "\n",
    "* **`Function's Role`:** The activation function's job is to decide whether the perceptron should `\"fire\"` or `\"activate,\"` and if so, what the final output should be.\n",
    "\n",
    "* **`Output Generation`:** For a traditional perceptron, this function is typically a **`step function`** or, in modern neural networks, often a **`Sigmoid`** function for binary classification.\n",
    "\n",
    "    * **`Step Function`:** If the weighted sum ($z$) is greater than a certain threshold (e.g., $0$), the output is $\\mathbf{1}$; otherwise, the output is $\\mathbf{0}$ or $\\mathbf{-1}$.\n",
    "\n",
    "* The value produced by the activation function is the **`final output`** or **`prediction`** of the single perceptron for the given input data. This output is then compared to the actual correct answer to calculate the error, which is used in the next phase, backpropagation (learning)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91e4b2a",
   "metadata": {},
   "source": [
    "---\n",
    "----\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270664e9",
   "metadata": {},
   "source": [
    "**Dataset Representation:**    \n",
    "We have $7$ samples with $3$ input features each:\n",
    "\n",
    "```py \n",
    "      Sample 1: [x₁₁, x₁₂, x₁₃]\n",
    "      Sample 2: [x₂₁, x₂₂, x₂₃]\n",
    "      Sample 3: [x₃₁, x₃₂, x₃₃]\n",
    "      Sample 4: [x₄₁, x₄₂, x₄₃]\n",
    "      Sample 5: [x₅₁, x₅₂, x₅₃]\n",
    "      Sample 6: [x₆₁, x₆₂, x₆₃]\n",
    "      Sample 7: [x₇₁, x₇₂, x₇₃]\n",
    "```\n",
    "\n",
    "**Network Architecture:**\n",
    "   - **Input layer**: $3$ features\n",
    "   - **Output layer**: $1$ perceptron with sign activation function\n",
    "   - **Activation function**: $sign(z) = +1$ if $z ≥ 0$, otherwise $-1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fa95ba",
   "metadata": {},
   "source": [
    "**Weight Representation:**\n",
    "\n",
    "Following PyTorch standards, weights are stored as:\n",
    "- **Weight matrix W**: shape $(1, 4)$ = `(out_features, in_features_with_bias)`\n",
    "- $W = [w₀, w₁, w₂, w₃]$\n",
    "  - $w₀$ = bias term\n",
    "  - $w₁, w₂, w₃$ = weights for the three input features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557c7345",
   "metadata": {},
   "source": [
    "## **Forward Propagation - One Complete Epoch (Processing All 7 Samples):**\n",
    "\n",
    "#### **Sample 1:**\n",
    "\n",
    "**Step 1: Prepare Input (Row Vector)**\n",
    "- Original input: $X₁ = [x₁₁, x₁₂, x₁₃]$ with shape $(1, 3)$\n",
    "\n",
    "**Step 2: Bias Augmentation (Prepend 1)**\n",
    "- Augmented input: $\\mathbf{X}_{1_{\\text{aug}}} = [1, x_{11}, x_{12}, x_{13}]$ with shape $(1, 4)$\n",
    "\n",
    "**Step 3: Linear Transformation**\n",
    "- Compute: $z₁ = \\mathbf{X}_{1_{\\text{aug}}} @ W^T$  \n",
    "\n",
    "- $z₁ = [1, x₁₁, x₁₂, x₁₃] @ [w₀, w₁, w₂, w₃]^T$\n",
    "\n",
    "- $z₁ = 1·w₀ + x₁₁·w₁ + x₁₂·w₂ + x₁₃·w₃$\n",
    "\n",
    "- $z₁ = w₀ + w₁x₁₁ + w₂x₁₂ + w₃x₁₃$\n",
    "\n",
    "**Step 4: Apply Activation Function:**\n",
    "- $o_1 = \\operatorname{sign}(z_1) = \n",
    "\\begin{cases}\n",
    "+1 & \\text{if } z_1 \\geq 0 \\\\\n",
    "-1 & \\text{if } z_1 < 0\n",
    "\\end{cases}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c4a2d0",
   "metadata": {},
   "source": [
    "#### **Sample 2:**\n",
    "\n",
    "**Step 1: Prepare Input**\n",
    "- $X₂ = [x₂₁, x₂₂, x₂₃]$\n",
    "\n",
    "**Step 2: Bias Augmentation**\n",
    "- $\\mathbf{X}_{2_{\\text{aug}}} = [1, x_{21}, x_{22}, x_{23}]$ Or, $\\tilde{\\mathbf{x}}^{(2)} = [1, x^{(2)}_1, x^{(2)}_2, x^{(2)}_3]^\\top$ \n",
    "\n",
    "**Step 3: Linear Transformation**\n",
    "- $z_2 = \\mathbf{X}_{2_{\\text{aug}}} @ \\mathbf{W}^\\top$\n",
    "- $z_2 = w_0 + w_1 x_2^1 + w_2 x_2^2 + w_3 x_2^3$\n",
    "\n",
    "**Step 4: Apply Activation**\n",
    "- $o₂ = sign(z₂)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a64f76",
   "metadata": {},
   "source": [
    "#### **Sample 3:**\n",
    "\n",
    "**Step 1: Prepare Input**\n",
    "- $\\mathbf{X}_3 = [x_{31}, x_{32}, x_{33}]$\n",
    "\n",
    "**Step 2: Bias Augmentation**\n",
    "- $\\mathbf{X}_{3_{\\text{aug}}} = [1, x_{31}, x_{32}, x_{33}]$ Or, $\\tilde{\\mathbf{x}}^{(3)} = [1, x^{(3)}_1, x^{(3)}_2, x^{(3)}_3]^\\top$ \n",
    "\n",
    "**Step 3: Linear Transformation**\n",
    "- $z_3 = \\mathbf{X}_{3_{\\text{aug}}} @ \\mathbf{W}^\\top$\n",
    "- $z_3 = w_0 + w_1 x_3^1 + w_2 x_3^2 + w_3 x_3^3$\n",
    "\n",
    "**Step 4: Apply Activation**\n",
    "- $o_3 = sign(z_3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d3a752",
   "metadata": {},
   "source": [
    "#### **Sample 4:**\n",
    "\n",
    "**Step 1: Prepare Input:**\n",
    "- $\\mathbf{X}_4 = [x_{41}, x_{42}, x_{43}]$\n",
    "\n",
    "**Step 2: Bias Augmentation:**\n",
    "- $\\mathbf{X}_{4_{\\text{aug}}} = [1, x_{41}, x_{42}, x_{43}]$ Or, $\\tilde{\\mathbf{x}}^{(4)} = [1, x^{(4)}_1, x^{(4)}_2, x^{(4)}_3]^\\top$ \n",
    "\n",
    "**Step 3: Linear Transformation:**\n",
    "- $z_4 = \\mathbf{X}_{4_{\\text{aug}}} @ \\mathbf{W}^\\top$\n",
    "- $z_4 = w_0 + w_1 x_4^1 + w_2 x_4^2 + w_3 x_4^3$\n",
    "\n",
    "**Step 4: Apply Activation:**\n",
    "- $o_4 = \\text{sign}(z_4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9d7da0",
   "metadata": {},
   "source": [
    "#### **Sample 5:**\n",
    "\n",
    "**Step 1: Prepare Input:**\n",
    "- $\\mathbf{X}_5 = [x_{51}, x_{52}, x_{53}]$\n",
    "\n",
    "**Step 2: Bias Augmentation:**\n",
    "- $\\mathbf{X}_{5_{\\text{aug}}} = [1, x_{51}, x_{52}, x_{53}]$ Or, $\\tilde{\\mathbf{x}}^{(5)} = [1, x^{(5)}_1, x^{(5)}_2, x^{(5)}_3]^\\top$ \n",
    "\n",
    "**Step 3: Linear Transformation:**\n",
    "- $z_5 = \\mathbf{X}_{5_{\\text{aug}}} @ \\mathbf{W}^\\top$\n",
    "- $z_5 = w_0 + w_1 x_5^1 + w_2 x_5^2 + w_3 x_5^3$\n",
    "\n",
    "**Step 4: Apply Activation:**\n",
    "- $o_5 = \\text{sign}(z_5)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f183563",
   "metadata": {},
   "source": [
    "#### **Sample 6:**\n",
    "\n",
    "**Step 1: Prepare Input**\n",
    "- $X₆ = [x₆₁, x₆₂, x₆₃]$\n",
    "\n",
    "**Step 2: Bias Augmentation**\n",
    "- $\\mathbf{X}_{6_{\\text{aug}}} = [1, x_{61}, x_{62}, x_{63}]$ Or, $\\tilde{\\mathbf{x}}^{(6)} = [1, x^{(6)}_1, x^{(6)}_2, x^{(6)}_3]^\\top$ \n",
    "\n",
    "**Step 3: Linear Transformation**\n",
    "- $z₆ = \\mathbf{X}_{6_{\\text{aug}}} @ \\mathbf{W}^\\top$\n",
    "-  $z₆ = w₀ + w₁x₆₁ + w₂x₆₂ + w₃x₆₃$\n",
    "\n",
    "**Step 4: Apply Activation**\n",
    "- $o₆ = \\text{sign}(z₆)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc6dfcf0",
   "metadata": {},
   "source": [
    "#### **Sample 7:**\n",
    "\n",
    "**Step 1: Prepare Input:**\n",
    "- $X₇ = [x₇₁, x₇₂, x₇₃]$\n",
    "\n",
    "**Step 2: Bias Augmentation:**\n",
    "- $\\mathbf{X}_{7_{\\text{aug}}} = [1, x_{71}, x_{72}, x_{73}]$ Or, $\\tilde{\\mathbf{x}}^{(7)} = [1, x^{(7)}_1, x^{(7)}_2, x^{(7)}_3]^\\top$ \n",
    "\n",
    "**Step 3: Linear Transformation:**\n",
    "- $z₇ = \\mathbf{X}_{7_{\\text{aug}}} @ \\mathbf{W}^\\top$\n",
    "- $z₇ = w₀ + w₁x₇₁ + w₂x₇₂ + w₃x₇₃$\n",
    "\n",
    "**Step 4: Apply Activation:**\n",
    "- $o₇ = \\text{sign}(z₇)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5fbdd2",
   "metadata": {},
   "source": [
    "After processing all 7 samples sequentially, we have:\n",
    "\n",
    "| Sample | Input | Weighted Sum ($z$) | Output ($o$) |\n",
    "|--------|-------|------------------|------------|\n",
    "| 1 | $[x₁₁, x₁₂, x₁₃]$ | $w₀ + w₁x₁₁ + w₂x₁₂ + w₃x₁₃$ | $o₁ = \\text{sign}(z₁)$ |\n",
    "| 2 | $[x₂₁, x₂₂, x₂₃]$ | $w₀ + w₁x₂₁ + w₂x₂₂ + w₃x₂₃$ | $o₂ = \\text{sign}(z₂)$ |\n",
    "| 3 | $[x₃₁, x₃₂, x₃₃]$ | $w₀ + w₁x₃₁ + w₂x₃₂ + w₃x₃₃$ | $o₃ = \\text{sign}(z₃)$ |\n",
    "| 4 | $[x₄₁, x₄₂, x₄₃]$ | $w₀ + w₁x₄₁ + w₂x₄₂ + w₃x₄₃$ | $o₄ = \\text{sign}(z₄)$ |\n",
    "| 5 | $[x₅₁, x₅₂, x₅₃]$ | $w₀ + w₁x₅₁ + w₂x₅₂ + w₃x₅₃$ | $o₅ = \\text{sign}(z₅)$ |\n",
    "| 6 | $[x₆₁, x₆₂, x₆₃]$ | $w₀ + w₁x₆₁ + w₂x₆₂ + w₃x₆₃$ | $o₆ = \\text{sign}(z₆)$ |\n",
    "| 7 | $[x₇₁, x₇₂, x₇₃]$ | $w₀ + w₁x₇₁ + w₂x₇₂ + w₃x₇₃$ | $o₇ = sign(z₇)$ |\n",
    "\n",
    "**One epoch is complete!** The perceptron has made predictions for all 7 samples in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d94f1a2",
   "metadata": {},
   "source": [
    "**Notes:** \n",
    "\n",
    "1. **`Sequential Processing`**: In one epoch, we process each sample one at a time (sample-by-sample or \"online\" learning)\n",
    "\n",
    "2. **`Row Vector Convention`**: Each input sample is a row vector, following PyTorch standards\n",
    "\n",
    "3. **`Bias Augmentation`**: We prepend 1 to each input, allowing the bias term to be treated as a regular weight\n",
    "\n",
    "4. **`Matrix Dimensions`**: \n",
    "   - Input after augmentation: $(1, 4)$\n",
    "   - Weights transposed: $(4, 1)$\n",
    "   - Result: $(1, 4) × (4, 1) = (1, 1)$ scalar output\n",
    "\n",
    "5. **`Sign Activation`**: Creates a binary classifier outputting $+1$ or $-1$\n",
    "\n",
    "6. **`No Learning Yet`**: During this forward pass, weights remain constant. **`Learning`** (weight updates) would happen during **`backpropagation`**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
