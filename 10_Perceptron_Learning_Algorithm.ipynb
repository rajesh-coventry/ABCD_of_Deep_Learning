{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd4490cc",
   "metadata": {},
   "source": [
    "# **Perceptron Learning Algorithm:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1dc0636",
   "metadata": {},
   "source": [
    "### **The Biological Inspiration and Mathematical Abstraction:**\n",
    "\n",
    "A perceptron is a mathematical model inspired by biological neurons.\n",
    "\n",
    "**Biological neuron:**\n",
    "- Receives signals from multiple dendrites (inputs)\n",
    "- Each connection has a strength (weight)\n",
    "- Neuron fires if total signal exceeds threshold\n",
    "\n",
    "**Mathematical abstraction:**\n",
    "- Inputs: $x₁, x₂, ..., xₙ$\n",
    "- Weights: $w₁, w₂, ..., wₙ$\n",
    "- Threshold: $θ$\n",
    "- Output: fires ($1$) or doesn't fire ($0$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4fc376",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d4ffed",
   "metadata": {},
   "source": [
    "## **Mathematical Model of Perceptron:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b584d4",
   "metadata": {},
   "source": [
    "#### **Step 1: Weighted Sum:**\n",
    "\n",
    "First, we compute how much total \"signal\" the neuron receives:\n",
    "\n",
    "> $$z = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n$$\n",
    "In vector notation:\n",
    "\n",
    "> $$z = \\mathbf{w}^\\top \\mathbf{x}$$\n",
    "\n",
    "Each input is multiplied by its weight (importance), then we sum everything up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcad1b10",
   "metadata": {},
   "source": [
    "#### **Step 2: Adding the Threshold:**\n",
    "\n",
    "The neuron fires only if the weighted sum exceeds a threshold $θ$:\n",
    "\n",
    "> $$\\text{output} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\mathbf{w}^\\top \\mathbf{x} \\geq \\theta \\\\\n",
    "0 & \\text{if } \\mathbf{w}^\\top \\mathbf{x} < \\theta\n",
    "\\end{cases}$$\n",
    "\n",
    "But, having a separate threshold is inconvenient mathematically. To solve this issue, we convert threshold to bias by moving it to the left side:\n",
    "\n",
    "> $\\mathbf{w}^\\top \\mathbf{x} \\geq \\theta$    \n",
    "> \n",
    "> $\\mathbf{w}^\\top \\mathbf{x} - \\theta \\geq 0$\n",
    "\n",
    "Define: **$b = -θ$** (bias term)\n",
    "\n",
    "> $wᵀx + b ≥ 0$\n",
    "\n",
    "**Now our model is:**\n",
    "\n",
    "> $$\\text{output} = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } \\mathbf{w}^\\top \\mathbf{x} + b \\geq 0 \\\\\n",
    "0 & \\text{if } \\mathbf{w}^\\top \\mathbf{x} + b < 0\n",
    "\\end{cases}$$\n",
    "\n",
    "Instead of asking `\"does signal exceed threshold?\"`, we ask `\"is signal plus bias positive?\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e68b5f",
   "metadata": {},
   "source": [
    "#### **Step 3: The Activation Function:**\n",
    "\n",
    "We can write this decision compactly using the **`sign function`**:\n",
    "\n",
    "> $$\\operatorname{sign}(z) = \n",
    "\\begin{cases}\n",
    "+1 & \\text{if } z > 0 \\\\\n",
    "-1 & \\text{if } z < 0 \\\\\n",
    "0 & \\text{if } z = 0\n",
    "\\end{cases}$$ \n",
    "\n",
    "**For binary classification, we use labels {$-1, +1$} instead of {$0, 1$}:**\n",
    "\n",
    "> $$\\hat{y} = \\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x} + b)$$\n",
    "\n",
    "**Why {$-1, +1$},** Because this makes the mathematics cleaner. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f5877a",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2eda0a",
   "metadata": {},
   "source": [
    "## **The Classification Problem:**\n",
    "\n",
    "**Given:** Training dataset with $N$ examples\n",
    "\n",
    "> $\\mathcal{D} = \\{(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), \\dots, (\\mathbf{x}_n, y_n)\\}$  \n",
    "\n",
    "Where:\n",
    "- **$xᵢ$** $∈ ℝⁿ$ (feature vector)\n",
    "- **$yᵢ$** $∈ {-1, +1}$ (true label)\n",
    "\n",
    "**Goal:** Find weights **w** and bias **b** such that:\n",
    "\n",
    "> $$\\operatorname{sign}(\\mathbf{w}^\\top \\mathbf{x}_i + b) = y_i \\quad \\text{for all } i = 1, 2, \\dots, N$$\n",
    "\n",
    "We want our perceptron to correctly classify every training example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b148b9",
   "metadata": {},
   "source": [
    "#### **Geometric Interpretation:**\n",
    "\n",
    "The equation **$wᵀx + b = 0$** defines a **`hyperplane`** (decision boundary):   \n",
    "   - **In 2D:** This is a line\n",
    "   - **In 3D:** This is a plane  \n",
    "   - **In nD:** This is an ($n-1$)-dimensional hyperplane\n",
    "\n",
    "**The hyperplane divides space into two regions:**\n",
    "   - **Region 1:** $wᵀx + b > 0$ → predict class $+1$\n",
    "   - **Region 2:** $wᵀx + b < 0$ → predict class $-1$\n",
    "\n",
    "**The weight vector $w$ is perpendicular (normal) to this hyperplane.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f21ae27",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e981da",
   "metadata": {},
   "source": [
    "## **Understanding Classification Correctness:**\n",
    "\n",
    "**When is a Point Correctly Classified?**\n",
    "\n",
    "This is a crucial insight.\n",
    "\n",
    "**Case 1: Point belongs to class $+1$ ($yᵢ = +1$)**\n",
    "\n",
    "For correct classification, we need:\n",
    "\n",
    "> $wᵀxᵢ + b > 0$\n",
    "\n",
    "**Case 2: Point belongs to class $-1$ ($yᵢ = -1$)**\n",
    "\n",
    "For correct classification, we need:\n",
    "\n",
    "> $wᵀxᵢ + b < 0$ \n",
    "\n",
    "**Multiply both sides by $yᵢ$:**\n",
    "\n",
    "**Case 1:** $yᵢ = +1$\n",
    "\n",
    "> $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) = (+1)(\\mathbf{w}^\\top \\mathbf{x}_i + b) = \\mathbf{w}^\\top \\mathbf{x}_i + b > 0 \\quad \\checkmark$\n",
    "\n",
    "\n",
    "**Case 2:** $yᵢ = -1$\n",
    "\n",
    "> $y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) = (-1)(\\mathbf{w}^\\top \\mathbf{x}_i + b) = -(\\mathbf{w}^\\top \\mathbf{x}_i + b) > 0 \\quad \\checkmark$ \n",
    "\n",
    "Since we need $wᵀxᵢ + b < 0$, multiplying by $-1$ gives:\n",
    "\n",
    "> $-(wᵀxᵢ + b) > 0$\n",
    "\n",
    "**Universal correctness condition:**\n",
    "\n",
    "> $yᵢ(wᵀxᵢ + b) > 0$  ⟺  point $i$ is correctly classified\n",
    "\n",
    "**This is beautiful!** The product of true label and prediction score tells us everything:\n",
    "   - **Positive product** → correct classification\n",
    "   - **Negative or zero product** → incorrect classification (error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c933c3f9",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529f6c3f",
   "metadata": {},
   "source": [
    "## **Defining the Loss Function:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53af6652",
   "metadata": {},
   "source": [
    "A loss function $L(w, b)$ measures how badly our current weights are performing. We want to minimize it.\n",
    "\n",
    "**For a correctly classified point:**\n",
    "\n",
    "> $yᵢ(wᵀxᵢ + b) > 0$   (positive value)\n",
    "\n",
    "No loss! The point is already correct.\n",
    "\n",
    "**For a misclassified point:**\n",
    "\n",
    "> $yᵢ(wᵀxᵢ + b) ≤ 0$   (negative or zero value)\n",
    "\n",
    "We want to penalize this.\n",
    "\n",
    "**Perceptron loss for a single misclassified point:**\n",
    "\n",
    "> $loss = -yᵢ(wᵀxᵢ + b)$ \n",
    "\n",
    "**Why the negative sign?**\n",
    "   - For misclassified points: $yᵢ(wᵀxᵢ + b) < 0$ (negative number)\n",
    "   \n",
    "   - We want loss to be positive: $-yᵢ(wᵀxᵢ + b) > 0$\n",
    "   \n",
    "   - The more negative $yᵢ(wᵀxᵢ + b)$ is, the larger our loss\n",
    "\n",
    "**The loss is proportional to how wrong we are. Being slightly wrong gives small loss; being very wrong gives large loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2a6f69",
   "metadata": {},
   "source": [
    "**Total Loss Function:** Sum over all misclassified points:\n",
    "\n",
    "> $$L(\\mathbf{w}, b) = -\\sum_{i \\in M} y_i (\\mathbf{w}^\\top \\mathbf{x}_i + b)$$\n",
    "\n",
    "where $M = \\{i : y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\leq 0\\}$ is the set of misclassified samples.\n",
    "\n",
    "**`Objective`:** Minimize $L(w, b)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd9f8b",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5a9d29",
   "metadata": {},
   "source": [
    "## **Gradient Descent - The Optimization Framework:**\n",
    "\n",
    "#### **What is Gradient Descent?**\n",
    "\n",
    "Imagine you're on a mountain in fog and want to reach the valley (minimum). You can only see the slope beneath your feet. Strategy: take steps downhill!\n",
    "\n",
    "**Mathematically:**\n",
    "\n",
    "> $$\\boldsymbol{\\theta}_{\\text{new}} = \\boldsymbol{\\theta}_{\\text{old}} - \\eta \\cdot \\nabla L(\\boldsymbol{\\theta})$$\n",
    "\n",
    "Where:\n",
    "- $θ$ represents all parameters ($w$ and $b$)\n",
    "- $η$ (eta) is the **learning rate** (step size)\n",
    "- $∇L(θ)$ is the **gradient** (direction of steepest ascent)\n",
    "- Negative gradient points downhill (steepest descent)\n",
    "\n",
    "Move in the opposite direction of the gradient, scaled by learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9011d0",
   "metadata": {},
   "source": [
    "#### **Why Gradient Descent Works?**\n",
    "\n",
    "**Taylor expansion around current point:**\n",
    "\n",
    "Taylor's theorem provides a way to approximate a function near a point using its derivatives. The first-order Taylor expansion approximates a function linearly by using its value and gradient at a point, essentially creating a tangent plane (or line) that locally approximates the function's behavior. This is fundamental in optimization because it lets us predict how a function will change if we move in a particular direction.\n",
    "\n",
    "> $$L(\\boldsymbol{\\theta} + \\Delta\\boldsymbol{\\theta}) \\approx L(\\boldsymbol{\\theta}) + \\nabla L(\\boldsymbol{\\theta})^\\top \\Delta\\boldsymbol{\\theta}$$\n",
    "\n",
    "**Choose:** $Δθ = -η∇L(θ)$\n",
    "\n",
    "Then:\n",
    "\n",
    "> $$L(\\boldsymbol{\\theta} + \\Delta\\boldsymbol{\\theta}) \\approx L(\\boldsymbol{\\theta}) - \\eta \\|\\nabla L(\\boldsymbol{\\theta})\\|^2$$\n",
    "Since $η > 0$ and $||∇L(θ)||² ≥ 0$:\n",
    "\n",
    "> $$L(\\boldsymbol{\\theta} + \\Delta\\boldsymbol{\\theta}) \\leq L(\\boldsymbol{\\theta})$$\n",
    "**Loss decreases!** (for small enough $η$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f086d02",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b197ed",
   "metadata": {},
   "source": [
    "## **Computing the Gradient:**\n",
    "\n",
    "#### **Gradient with Respect to Weights ($w$):**\n",
    "\n",
    "For a single misclassified point, our loss is:\n",
    "\n",
    "> $$L = -y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)$$\n",
    "\n",
    "Take the derivative with respect to **$w$**:\n",
    "\n",
    "> $$\\frac{\\partial L}{\\partial \\mathbf{w}} = \\frac{\\partial}{\\partial \\mathbf{w}}\\left[-y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right]$$\n",
    "\n",
    "**Factor out the constant $-yᵢ$:**\n",
    "\n",
    "> $$\\frac{\\partial L}{\\partial \\mathbf{w}} = -y_i \\cdot \\frac{\\partial}{\\partial \\mathbf{w}}(\\mathbf{w}^\\top \\mathbf{x}_i + b)$$\n",
    "\n",
    "**The term $b$ doesn't depend on $w$, so it vanishes:**\n",
    "\n",
    "> $$\\frac{\\partial L}{\\partial \\mathbf{w}} = -y_i \\cdot \\frac{\\partial}{\\partial \\mathbf{w}}(\\mathbf{w}^\\top \\mathbf{x}_i)$$\n",
    "\n",
    "**Now we need:** $∂/∂w(wᵀxᵢ)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b9a88",
   "metadata": {},
   "source": [
    "Let's expand $wᵀxᵢ$:\n",
    "\n",
    "> $$\\mathbf{w}^\\top \\mathbf{x}_i = w_1 x_{1i} + w_2 x_{2i} + \\cdots + w_n x_{ni}$$\n",
    "\n",
    "Take partial derivatives:\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "\\frac{\\partial (\\mathbf{w}^\\top \\mathbf{x}_i)}{\\partial w_1} &= x_{1i} \\\\\n",
    "\\frac{\\partial (\\mathbf{w}^\\top \\mathbf{x}_i)}{\\partial w_2} &= x_{2i} \\\\\n",
    "&\\vdots \\\\\n",
    "\\frac{\\partial (\\mathbf{w}^\\top \\mathbf{x}_i)}{\\partial w_n} &= x_{ni}\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Collect into a vector:**\n",
    "\n",
    "> $\\frac{\\partial (\\mathbf{w}^\\top \\mathbf{x}_i)}{\\partial \\mathbf{w}} = [x_{1i}, x_{2i}, \\dots, x_{ni}]^\\top = \\mathbf{x}_i$ \n",
    "\n",
    "**Therefore:**\n",
    "\n",
    "> $$\\frac{\\partial L}{\\partial \\mathbf{w}} = -y_i \\mathbf{x}_i$$\n",
    "\n",
    "The gradient with respect to weights is negative true label times input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e66b4eaf",
   "metadata": {},
   "source": [
    "#### **Gradient with Respect to Bias ($b$):**\n",
    "\n",
    "Similarly:\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial b} &= \\frac{\\partial}{\\partial b}\\left[-y_i(\\mathbf{w}^\\top \\mathbf{x}_i + b)\\right] \\\\\n",
    "&= -y_i \\cdot \\frac{\\partial}{\\partial b}(\\mathbf{w}^\\top \\mathbf{x}_i + b)\n",
    "\\end{aligned}$$ \n",
    "\n",
    "**The term $wᵀxᵢ$ doesn't depend on $b$:**\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial b} &= -y_i \\cdot \\frac{\\partial}{\\partial b}(\\mathbf{w}^\\top \\mathbf{x}_i + b) \\\\\n",
    "&= -y_i \\cdot \\left[\\frac{\\partial}{\\partial b}(\\mathbf{w}^\\top \\mathbf{x}_i) + \\frac{\\partial b}{\\partial b}\\right] \\\\\n",
    "&= -y_i \\cdot \\left[0 + \\frac{\\partial b}{\\partial b}\\right] \\quad \\text{(since } \\mathbf{w}^\\top \\mathbf{x}_i \\text{ is constant w.r.t. } b\\text{)} \\\\\n",
    "&= -y_i \\cdot \\frac{\\partial b}{\\partial b} \\\\\n",
    "&= -y_i \\cdot 1 \\quad \\text{(since } \\frac{\\partial b}{\\partial b} = 1\\text{)} \\\\\n",
    "&= -y_i\n",
    "\\end{aligned}$$ \n",
    "\n",
    "The gradient with respect to bias is simply negative true label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827ac553",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00486a9",
   "metadata": {},
   "source": [
    "## **Deriving the Update Rule:**\n",
    "\n",
    "Using our gradient descent formula:\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "\\mathbf{w}_{\\text{new}} &= \\mathbf{w}_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial \\mathbf{w}} \\\\\n",
    "b_{\\text{new}} &= b_{\\text{old}} - \\eta \\cdot \\frac{\\partial L}{\\partial b}\n",
    "\\end{aligned}$$ \n",
    "\n",
    "**Substitute our gradients:**\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "\\mathbf{w}_{\\text{new}} &= \\mathbf{w}_{\\text{old}} - \\eta \\cdot (-y_i \\mathbf{x}_i) \\\\\n",
    "b_{\\text{new}} &= b_{\\text{old}} - \\eta \\cdot (-y_i)\n",
    "\\end{aligned}$$\n",
    "\n",
    "**Simplify (negative times negative is positive):**\n",
    "\n",
    "> $$\\begin{aligned}\n",
    "\\mathbf{w} &\\leftarrow \\mathbf{w} + \\eta \\cdot y_i \\mathbf{x}_i \\\\\n",
    "b &\\leftarrow b + \\eta \\cdot y_i\n",
    "\\end{aligned}$$\n",
    "\n",
    "**This is the Perceptron Update Rule!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "258efade",
   "metadata": {},
   "source": [
    "------\n",
    "---------\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f6b20a",
   "metadata": {},
   "source": [
    "## **Understanding the Update Geometrically:**\n",
    "\n",
    "Let's see what this update actually does:\n",
    "\n",
    "**Case 1: True label $yᵢ = +1$, but we predicted $-1$ $(wᵀxᵢ + b < 0)$:**\n",
    "\n",
    "Point is on the wrong side (negative side) but should be on positive side.\n",
    "\n",
    "Update:\n",
    "\n",
    "> $$\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot (+1) \\cdot \\mathbf{x}_i = \\mathbf{w} + \\eta \\cdot \\mathbf{x}_i$$\n",
    "\n",
    "**Effect:**\n",
    "- We add $xᵢ$ to $w$ (move $w$ toward $xᵢ$)\n",
    "- This increases the dot product: $wᵀxᵢ$ becomes larger\n",
    "- Next time: $wᵀxᵢ + b$ is more likely to be positive ✓\n",
    "\n",
    "**`Geometric`:** We rotate the decision boundary toward the misclassified positive point.\n",
    "\n",
    "\n",
    "**Case 2: True label $yᵢ = -1$, but we predicted $+1$ ($wᵀxᵢ + b > 0$)**\n",
    "\n",
    "Point is on the wrong side (positive side) but should be on negative side.\n",
    "\n",
    "Update:\n",
    "\n",
    "> $\\mathbf{w} \\leftarrow \\mathbf{w} + \\eta \\cdot (-1) \\cdot \\mathbf{x}_i = \\mathbf{w} - \\eta \\cdot \\mathbf{x}_i$ \n",
    "\n",
    "**Effect:**\n",
    "- We subtract $xᵢ$ from $w$ (move $w$ away from $x$ᵢ)\n",
    "- This decreases the dot product: $wᵀxᵢ$ becomes smaller\n",
    "- Next time: $wᵀxᵢ + b$ is more likely to be negative ✓\n",
    "\n",
    "**`Geometric`:** We rotate the decision boundary away from the misclassified negative point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be308e2d",
   "metadata": {},
   "source": [
    "#### **Visualizing the Weight Update:**\n",
    "\n",
    "Think of $w$ as an arrow in feature space:\n",
    "\n",
    "```py \n",
    "      Before update: w (current direction)\n",
    "      After update:  w + ηyᵢxᵢ (adjusted direction)\n",
    "```\n",
    "\n",
    "**If $yᵢ = +1$:** We rotate $w$ toward $xᵢ$\n",
    "**If $yᵢ = -1$:** We rotate $w$ away from $xᵢ$\n",
    "\n",
    "**The magnitude $η$ controls how much we rotate.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c710070a",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "-------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6233c965",
   "metadata": {},
   "source": [
    "## **The Complete Algorithm:**\n",
    "\n",
    "```py \n",
    "    ─────────────────────────────────────────────────────────────\n",
    "    INPUT: \n",
    "      - Training data: D = {(x₁, y₁), (x₂, y₂), ..., (xₙ, yₙ)}\n",
    "      - Learning rate: η > 0\n",
    "      - Maximum epochs: T\n",
    "    ─────────────────────────────────────────────────────────────\n",
    "\n",
    "    INITIALIZE:\n",
    "      w ← 0  (weight vector, all zeros)\n",
    "      b ← 0  (bias, zero)\n",
    "\n",
    "    FOR epoch = 1 to T:\n",
    "        \n",
    "        mistakes ← 0  (count errors this epoch)\n",
    "    \n",
    "        FOR each training example (xᵢ, yᵢ):\n",
    "            \n",
    "            STEP 1: Compute activation\n",
    "            ────────────────────────────\n",
    "            z ← wᵀxᵢ + b\n",
    "            \n",
    "            STEP 2: Check classification\n",
    "            ────────────────────────────\n",
    "            IF yᵢ · z ≤ 0:  (misclassified)\n",
    "                \n",
    "                STEP 3: Update weights\n",
    "                ────────────────────────\n",
    "                w ← w + η · yᵢ · xᵢ\n",
    "                \n",
    "                STEP 4: Update bias\n",
    "                ────────────────────────\n",
    "                b ← b + η · yᵢ\n",
    "                \n",
    "                mistakes ← mistakes + 1\n",
    "        \n",
    "        IF mistakes == 0:\n",
    "            BREAK  (converged - no errors this epoch)\n",
    "\n",
    "    ─────────────────────────────────────────────────────────────\n",
    "    OUTPUT: w, b (learned parameters)\n",
    "    ─────────────────────────────────────────────────────────────\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e077793e",
   "metadata": {},
   "source": [
    "## **Algorithm Variants:**\n",
    "\n",
    "**Variant 1: Online (Stochastic) Update:**\n",
    "- Update after each misclassified point (as shown above)\n",
    "- More frequent updates\n",
    "- Can converge faster in practice\n",
    "\n",
    "**Variant 2: Batch Update:**\n",
    "- Accumulate gradients for all misclassified points in an epoch\n",
    "- Update once per epoch\n",
    "- More stable but slower\n",
    "\n",
    "**Variant 3: $η = 1$ (Unit Learning Rate)**\n",
    "```py \n",
    "         w ← w + yᵢxᵢ\n",
    "         b ← b + yᵢ\n",
    "```\n",
    "Often used because perceptron is scale-invariant (only direction of $w$ matters for classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad02f44a",
   "metadata": {},
   "source": [
    "-----------\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92105f78",
   "metadata": {},
   "source": [
    "## **Bias Augmentation (PyTorch Convention):**\n",
    "\n",
    "Instead of handling $w$ and $b$ separately, we can combine them into a single parameter vector.\n",
    "\n",
    "**Original formulation:**\n",
    "\n",
    "> $z = w₁x₁ + w₂x₂ + ... + wₙxₙ + b$ \n",
    "\n",
    "**Augment the input by prepending 1:**\n",
    "\n",
    "> $x̃ = [1, x₁, x₂, ..., xₙ]$  (length $n+1$)\n",
    "\n",
    "**Augment weights to include bias:**\n",
    "\n",
    "> $w̃ = [b, w₁, w₂, ..., wₙ]$  (length n+1)\n",
    "\n",
    "**Now:**\n",
    "\n",
    "> $z = w̃ᵀx̃ = b·1 + w₁x₁ + w₂x₂ + ... + wₙxₙ$\n",
    "\n",
    "**Same result, but unified representation!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d130bf9a",
   "metadata": {},
   "source": [
    "#### **Algorithm with Bias Augmentation:**\n",
    "\n",
    "```PY \n",
    "         INITIALIZE:\n",
    "         w̃ ← 0  (augmented weight vector, length n+1)\n",
    "\n",
    "         FOR each training example (xᵢ, yᵢ):\n",
    "            \n",
    "            STEP 1: Augment input\n",
    "            ─────────────────────\n",
    "            x̃ᵢ = [1, xᵢ]  (prepend 1)\n",
    "            \n",
    "            STEP 2: Compute activation\n",
    "            ────────────────────────────\n",
    "            z = w̃ᵀx̃ᵢ\n",
    "            \n",
    "            STEP 3: Check and update\n",
    "            ────────────────────────────\n",
    "            IF yᵢ · z ≤ 0:\n",
    "               w̃ ← w̃ + η · yᵢ · x̃ᵢ\n",
    "\n",
    "         OUTPUT: w̃\n",
    "```\n",
    "\n",
    "**`Benefit`:** Single unified update rule instead of two separate ones!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d24162b",
   "metadata": {},
   "source": [
    "**Data representation:**\n",
    "- Input (row vector): $x = [x₁, x₂, ..., xₙ]$, shape ($1, n$)\n",
    "- Augmented input: $x̃ = [1, x₁, x₂, ..., xₙ]$, shape ($1, n+1$)\n",
    "\n",
    "**Weights (stored as column for computation):**\n",
    "- $w̃$ stored as shape ($n+1, 1$) for matrix multiplication\n",
    "\n",
    "**Forward pass:**\n",
    "```python\n",
    "   # x̃ has shape (1, n+1)\n",
    "   # w̃ has shape (n+1, 1)\n",
    "   z = x̃ @ w̃  # Result: (1, 1)\n",
    "   prediction = sign(z)\n",
    "```\n",
    "\n",
    "**Update (when misclassified):**\n",
    "```python\n",
    "      # Convert row vector to column for addition\n",
    "      w̃ = w̃ + η * yᵢ * x̃ᵢ.T  # All shapes (n+1, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92dd420c",
   "metadata": {},
   "source": [
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "907ba661",
   "metadata": {},
   "source": [
    "## **Limitations and Extensions:**\n",
    "\n",
    "#### **When Perceptron Fails:**\n",
    "\n",
    "**1. Perceptron Fails when we Hve Non-linearly separable data:**\n",
    "\n",
    "If no line/hyperplane can separate the classes, perceptron **`never converges`** — it cycles indefinitely.\n",
    "\n",
    "**Classic example: $XOR$ problem:**\n",
    "\n",
    "```py \n",
    "      Input     Output\n",
    "      x₁  x₂  |  y\n",
    "      ──────────────\n",
    "      0   0   | -1\n",
    "      0   1   | +1\n",
    "      1   0   | +1\n",
    "      1   1   | -1\n",
    "``` \n",
    "\n",
    "No straight line in $2D$ can separate this!\n",
    "\n",
    "**Solution:** \n",
    "   - Use kernel methods (map to higher dimensions)\n",
    "   - Use multi-layer perceptrons (neural networks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817467cd",
   "metadata": {},
   "source": [
    "**2. Practical Issues and Limitations:**\n",
    "\n",
    "**Issue 1: Sensitivity to learning rate**\n",
    "- Too large → oscillates, unstable\n",
    "- Too small → slow convergence\n",
    "- Common practice: start with η = 1, decrease if unstable\n",
    "\n",
    "**Issue 2: Order of examples matters**\n",
    "- Different orderings can give different solutions\n",
    "- Solutions are not unique (any separating hyperplane works)\n",
    "- Common practice: shuffle data between epochs\n",
    "\n",
    "**Issue 3: Outliers**\n",
    "- Single outlier can make data non-separable\n",
    "- Perceptron keeps trying to fit it\n",
    "- Solution: Use regularization or robust variants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e56551",
   "metadata": {},
   "source": [
    "#### **Extensions on Perceptron:**\n",
    "\n",
    "**1. Pocket Algorithm (for non-separable data):**\n",
    "```python\n",
    "      Keep track of best weights seen so far (fewest errors)\n",
    "      Even if we don't converge, return best solution\n",
    "```\n",
    "\n",
    "**2. Voted Perceptron:**\n",
    "```py \n",
    "      Keep all weight vectors during training\n",
    "      Prediction: weighted vote based on survival time\n",
    "      More robust, better generalization\n",
    "```\n",
    "\n",
    "**3. Margin Perceptron:**\n",
    "```py \n",
    "      Don't just correct errors, maximize margin\n",
    "      Leads to Support Vector Machines (SVM)\n",
    "```\n",
    "\n",
    "**4. Multi-class Perceptron:**\n",
    "```py \n",
    "      One weight vector per class\n",
    "      Predict class with highest score\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73812059",
   "metadata": {},
   "source": [
    "--------------\n",
    "------\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76ae0af",
   "metadata": {},
   "source": [
    "## **Example:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d567d307",
   "metadata": {},
   "source": [
    "**Data:** 4 points in 2D\n",
    "\n",
    "```py \n",
    "      x₁ = [3, 4],   y₁ = +1\n",
    "      x₂ = [1, 1],   y₂ = -1\n",
    "      x₃ = [2, 3],   y₃ = +1\n",
    "      x₄ = [0, 1],   y₄ = -1\n",
    "```\n",
    "\n",
    "**Initialize:** $w = [0, 0]ᵀ$, $b = 0$, $η = 1$\n",
    "\n",
    "#### **Epoch 1:**\n",
    "\n",
    "**Example 1: $(x₁, y₁) = ([3, 4], +1)$**\n",
    "\n",
    "```py \n",
    "         z = wᵀx₁ + b = 0·3 + 0·4 + 0 = 0\n",
    "         y₁·z = (+1)·0 = 0 ≤ 0  ✗ Misclassified!\n",
    "\n",
    "         Update:\n",
    "         w ← w + y₁x₁ = [0, 0] + (+1)[3, 4] = [3, 4]\n",
    "         b ← b + y₁ = 0 + (+1) = 1\n",
    "```\n",
    "\n",
    "Current: $w = [3, 4]$, $b = 1$\n",
    "\n",
    "**Example 2: $(x₂, y₂) = ([1, 1], -1)$**\n",
    "\n",
    "```py \n",
    "      z = wᵀx₂ + b = 3·1 + 4·1 + 1 = 8\n",
    "      y₂·z = (-1)·8 = -8 < 0  ✗ Misclassified!\n",
    "\n",
    "      Update:\n",
    "      w ← [3, 4] + (-1)[1, 1] = [3-1, 4-1] = [2, 3]\n",
    "      b ← 1 + (-1) = 0\n",
    "```\n",
    "\n",
    "Current: $w = [2, 3]$, $b = 0$\n",
    "\n",
    "**Example 3: $(x₃, y₃) = ([2, 3], +1)$**\n",
    "\n",
    "```py \n",
    "         z = 2·2 + 3·3 + 0 = 4 + 9 = 13\n",
    "         y₃·z = (+1)·13 = 13 > 0  ✓ Correct!\n",
    "\n",
    "         No update.\n",
    "```\n",
    "\n",
    "Current: $w = [2, 3]$, $b = 0$\n",
    "\n",
    "\n",
    "**Example 4: $(x₄, y₄) = ([0, 1], -1)$**\n",
    "\n",
    "```py \n",
    "         z = 2·0 + 3·1 + 0 = 3\n",
    "         y₄·z = (-1)·3 = -3 < 0  ✗ Misclassified!\n",
    "\n",
    "         Update:\n",
    "         w ← [2, 3] + (-1)[0, 1] = [2, 3-1] = [2, 2]\n",
    "         b ← 0 + (-1) = -1\n",
    "```\n",
    "\n",
    "Current: $w = [2, 2]$, $b = -1$\n",
    "\n",
    "**Epoch 1 complete:** 3 mistakes\n",
    "\n",
    "**Epoch 2:**\n",
    "\n",
    "Repeat with $w = [2, 2]$, $b = -1$ .......\n",
    "\n",
    "(Continue until no mistakes in a complete epoch)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
