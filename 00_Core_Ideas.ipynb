{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "164fcc0c",
   "metadata": {},
   "source": [
    "# **Foundational Concepts:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7b465a",
   "metadata": {},
   "source": [
    "### **1. Biological Neuron Intuition**\n",
    "\n",
    "Understand the biological origin of neural networks.\n",
    "\n",
    "* What is a neuron (dendrites, soma, axon)?\n",
    "* What is a synapse?\n",
    "* What does it mean for a neuron to “fire”?\n",
    "* Electrical signals → spikes\n",
    "* Neurons combine signals and produce an output\n",
    "\n",
    "**Get a mental model like this:**\n",
    "\n",
    "> Inputs (signals) → Summation → Threshold → Output (spike)\n",
    "\n",
    "This intuition maps directly to the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe330f7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f338646",
   "metadata": {},
   "source": [
    "### **2. Hebbian Learning (Foundational Learning Rules)**\n",
    "\n",
    "This is the classic biological rule, *not used in deep learning today*, but fundamental conceptually.\n",
    "\n",
    "* **Hebb’s rule**: “Neurons that fire together wire together”\n",
    "* Synaptic strength increases when input and output are active together\n",
    "* Understanding correlation-based learning\n",
    "* The idea of weights representing connection strength\n",
    "\n",
    "* **Oja’s Rule** (to see how Hebb’s rule is stabilized against weight explosion)\n",
    "\n",
    "Understand how a very simple system can *adapt* its weights based on co-activation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0316c2b",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba9a43b",
   "metadata": {},
   "source": [
    "### **3. Delta Rule / Gradient Descent Intuition**\n",
    "\n",
    "This is where learning becomes mathematical and enters the world of modern neural networks.\n",
    "\n",
    "* Idea of an “error” between prediction and target\n",
    "* Weights should change to reduce error\n",
    "* Very light intro to gradients (only the idea, no heavy math)\n",
    "* Delta Rule (or LMS rule):\n",
    "\n",
    "  > Δw = learning_rate × error × input\n",
    "\n",
    "Understand that learning can be **guided by error**, not just correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb98824c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac9e387",
   "metadata": {},
   "source": [
    "### **4. Linear Classifiers**\n",
    "\n",
    "Before understanding a perceptron, understand what problem it actually solves.\n",
    "\n",
    "* What is classification?\n",
    "* What is a **decision boundary**?\n",
    "* What is **linear separability**?\n",
    "* Plotting simple 2D points and imagining a line separating classes\n",
    "* Why some problems cannot be solved with a single line (e.g., XOR)\n",
    "\n",
    "Understand that a perceptron is essentially a machine that tries to draw a separating line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183440ae",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d50b6",
   "metadata": {},
   "source": [
    "### **5. Elements of an Artificial Neuron**\n",
    "\n",
    "Now study how we model a neuron mathematically.\n",
    "\n",
    "* Inputs (x₁, x₂, …)\n",
    "* Weights (w₁, w₂, …)\n",
    "* Weighted sum: **w · x**\n",
    "* Bias term (b) → shifts the decision boundary\n",
    "* Activation function (threshold/step)\n",
    "\n",
    "Understand how a neuron computes an output value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f9633f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8caf6d5",
   "metadata": {},
   "source": [
    "### **6. Perceptron Learning Algorithm:**\n",
    "\n",
    "This is where everything comes together.\n",
    "\n",
    "- Perceptron equation:\n",
    "   > output = step(w · x + b)\n",
    "* Interpretation of the weights and bias\n",
    "* Perceptron weight update rule\n",
    "* Why the perceptron converges only if data is linearly separable\n",
    "* Limitations (cannot solve XOR)\n",
    "\n",
    "**Be capable of:**\n",
    "\n",
    "* manually computing the output of a perceptron\n",
    "* understanding why weights change\n",
    "* training a perceptron on simple OR/AND tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4a5951",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e285206c",
   "metadata": {},
   "source": [
    "1. **Biological Neuron → concept of signals and thresholds**\n",
    "\n",
    "2. **Hebbian Learning → intuition behind weight strengthening**\n",
    "\n",
    "3. **Delta Rule → error-based learning concept**\n",
    "\n",
    "4. **Linear Classifiers → what problem the perceptron solves**\n",
    "\n",
    "5. **Artificial Neuron Components → mathematical neuron model**\n",
    "\n",
    "6. **Perceptron Algorithm → first real working neural network**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
