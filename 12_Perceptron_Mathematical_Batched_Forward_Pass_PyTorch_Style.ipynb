{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "afc70163",
   "metadata": {},
   "source": [
    "# **Perceptron: Mathematical Batched Forward Pass (PyTorch):**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aac566b",
   "metadata": {},
   "source": [
    "**`Batching`** is a fundamental technique in deep learning training that involves dividing the entire training dataset into smaller, manageable subsets of data called **`batches`** (or **`mini-batches`**).\n",
    "\n",
    "The **`Batch Size`** is a crucial hyperparameter that specifies the exact number of training examples contained in one batch. This batch of data is then processed through the neural network together in a single iteration **(one forward pass and one backward pass)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f680ff",
   "metadata": {},
   "source": [
    "### **Why We Do Batching?**\n",
    "\n",
    "Batching is essential because it strikes a practical and computational balance between the two extremes of using a single data point or the entire dataset at once. \n",
    "\n",
    "**The main reasons for batching are:**\n",
    "\n",
    "1. **`Memory Efficiency (The biggest reason)`:** Large, modern deep learning datasets (millions of images, terabytes of text) often cannot fit entirely into the memory of a single GPU or CPU. Batching allows you to process the data in chunks that fit within the hardware's capacity.\n",
    "\n",
    "2. **`Computational Efficiency & Parallelism`:** Modern hardware accelerators like GPUs are optimized for parallel computation. Processing a small batch of inputs simultaneously (in parallel) is vastly more efficient than processing one example at a time. This significantly reduces the total training time.\n",
    "\n",
    "3. **`Gradient Stability vs. Noise`:**  \n",
    "   * Using the entire dataset gives a very stable, accurate gradient, but it's slow.   \n",
    "    * Using a single example gives a very noisy, unstable gradient, but it's fast.   \n",
    "    * Batching uses a small group of samples (e.g., $32, 64, 128$) to compute a **`better, less noisy estimate of the true gradient`** than a single example, while remaining computationally efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441242e4",
   "metadata": {},
   "source": [
    "### **Types of Batching and Gradient Descent:**\n",
    "\n",
    "The choice of batch size directly defines the type of **`Gradient Descent`** optimization algorithm being used:\n",
    "\n",
    "| Batching Type | Batch Size | Gradient Calculation | Update Frequency (per Epoch) | Trade-offs |  |\n",
    "| --- | --- | --- | --- | --- | --- |\n",
    "| **1. Batch Gradient Descent ($BGD$)** | **Entire Dataset ($N$)** | Uses the average loss/gradient from **all** N samples. | **$1$** | **Pros:** Very stable, true gradient direction. **Cons:** Extremely slow for large N, high memory usage, can get stuck in sharp local minima. |  |\n",
    "| **2. Stochastic Gradient Descent ($SGD$)** | **$1$** | Uses the loss/gradient from **one single sample** at a time. | **$N$** (Number of training samples) | **Pros:** Very fast iterations, low memory, gradient noise helps escape local minima. **Cons:** Highly noisy and unstable convergence path, low computational efficiency due to poor GPU utilization. |  |\n",
    "| **3. Mini-Batch Gradient Descent ($MBGD$)** | **$1 < \\text{Batch Size} < N$** (e.g., $32, 64, 128$) | Uses the average loss/gradient from the small subset of samples in the mini-batch. | **$N / \\text{Batch Size}$** | **Pros:** Excellent balance of stability and speed, highest computational efficiency, and is the **standard method** used in deep learning. |  |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238d979e",
   "metadata": {},
   "source": [
    "### **Does Gradient Descent Depend on the Type of Batching?**\n",
    "\n",
    "**Yes.** \n",
    "\n",
    "The type of batching (i.e., the batch size) is what fundamentally defines which of the three primary **`Gradient Descent`** variants you are using.\n",
    "\n",
    "The gradient descent algorithm is the process of updating the model's weights ($\\theta$) using the following formula:\n",
    "\n",
    "**Where**:   \n",
    "   * $\\theta$: The model's weights and biases.\n",
    "   * $\\eta$: The learning rate.\n",
    "   * $\\nabla J(\\theta)$: The **`Gradient`** of the loss function J with respect to the weights.\n",
    "\n",
    "The core difference is in **how the gradient ($\\nabla J(\\theta)$) is calculated:**\n",
    "\n",
    "* **$BGD$** computes the gradient using **all** samples, giving a highly accurate, deterministic direction.\n",
    "\n",
    "* **$SGD$** computes the gradient using **one** sample, giving a highly noisy, stochastic (random-like) direction.\n",
    "\n",
    "* **$MBGD$** computes the gradient using a **subset** of samples, providing a fast and reasonable approximation of the true gradient.\n",
    "\n",
    "The choice of `batch size` is a critical hyperparameter that dictates the `computational efficiency`, the `memory footprint`, and the `stability` of the entire training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97df0607",
   "metadata": {},
   "source": [
    "---------\n",
    "---------\n",
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b045e134",
   "metadata": {},
   "source": [
    "**Dataset Representation ($7$ samples, $3$ features each):** \n",
    "\n",
    "```py\n",
    "        X = [x₁₁  x₁₂  x₁₃]\n",
    "            [x₂₁  x₂₂  x₂₃]\n",
    "            [x₃₁  x₃₂  x₃₃]\n",
    "            [x₄₁  x₄₂  x₄₃]\n",
    "            [x₅₁  x₅₂  x₅₃]\n",
    "            [x₆₁  x₆₂  x₆₃]\n",
    "            [x₇₁  x₇₂  x₇₃]\n",
    "```\n",
    "Shape: $(7, 3)$ = `(batch_size, features)`\n",
    "\n",
    "**True Labels (Binary Output):** \n",
    "\n",
    "```py \n",
    "        Y_true = [y₁]\n",
    "                 [y₂]\n",
    "                 [y₃]\n",
    "                 [y₄]\n",
    "                 [y₅]\n",
    "                 [y₆]\n",
    "                 [y₇]\n",
    "```\n",
    "Shape: $(7, 1)$ = `(batch_size, 1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7992bfa",
   "metadata": {},
   "source": [
    "**Network Architecture:**  \n",
    "   - **Input layer**: $3$ features\n",
    "   - **Output layer**: $1$ neuron\n",
    "   - **Activation function**: $ReLU(z) = max(0, z)$\n",
    "   - **Loss function**: `Squared Error Loss`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778ec50a",
   "metadata": {},
   "source": [
    "**Weight Representation (PyTorch Convention):**  \n",
    "   - **Weight matrix W**: shape $(1, 4)$ = `(out_features, in_features_with_bias)`\n",
    "   \n",
    "   - $W = [w₀, w₁, w₂, w₃]$\n",
    "     - $w₀ =$ bias term\n",
    "   \n",
    "     - $w₁, w₂, w₃ =$ weights for features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9376ca",
   "metadata": {},
   "source": [
    "**Batched Forward Propagation (One Epoch, Batch Size $= 7$):**\n",
    "\n",
    "**`Step 1`: Prepare Batch Input:**    \n",
    "Original batch input matrix:\n",
    "```py\n",
    "        X = [x₁₁  x₁₂  x₁₃]\n",
    "            [x₂₁  x₂₂  x₂₃]\n",
    "            [x₃₁  x₃₂  x₃₃]\n",
    "            [x₄₁  x₄₂  x₄₃]\n",
    "            [x₅₁  x₅₂  x₅₃]\n",
    "            [x₆₁  x₆₂  x₆₃]\n",
    "            [x₇₁  x₇₂  x₇₃]\n",
    "```\n",
    "Shape: $(7, 3)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ba34d8",
   "metadata": {},
   "source": [
    "**`Step 2`: Bias Augmentation (Prepend 1 as First Column):**    \n",
    "Augmented batch input:\n",
    "```py \n",
    "         X_aug = [1  x₁₁  x₁₂  x₁₃]\n",
    "                 [1  x₂₁  x₂₂  x₂₃]\n",
    "                 [1  x₃₁  x₃₂  x₃₃]\n",
    "                 [1  x₄₁  x₄₂  x₄₃]\n",
    "                 [1  x₅₁  x₅₂  x₅₃]\n",
    "                 [1  x₆₁  x₆₂  x₆₃]\n",
    "                 [1  x₇₁  x₇₂  x₇₃]\n",
    "```\n",
    "Shape: $(7, 4)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c437cf70",
   "metadata": {},
   "source": [
    "**`Step 3`: Linear Transformation (Batch Matrix Multiplication):**\n",
    "\n",
    "**Weight Matrix:**  \n",
    " \n",
    "> $W = [w₀ , w₁,  w₂ , w₃]$ \n",
    "\n",
    "Shape: $(1, 4)$\n",
    "\n",
    "**Transpose for Multiplication:**\n",
    "```py\n",
    "         W^T = [w₀]\n",
    "               [w₁]\n",
    "               [w₂]\n",
    "               [w₃]\n",
    "```\n",
    "Shape: $(4, 1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adbc54b",
   "metadata": {},
   "source": [
    "**Batch Computation:**\n",
    "```py \n",
    "         Z = X_aug @ W^T\n",
    "```\n",
    "Shape: $(7, 4) × (4, 1) = (7, 1)$\n",
    "\n",
    "**Expanded Calculation:**\n",
    "```py \n",
    "         Z = [1  x₁₁  x₁₂  x₁₃]   [w₀]\n",
    "             [1  x₂₁  x₂₂  x₂₃]   [w₁]\n",
    "             [1  x₃₁  x₃₂  x₃₃] @ [w₂]\n",
    "             [1  x₄₁  x₄₂  x₄₃]   [w₃]\n",
    "             [1  x₅₁  x₅₂  x₅₃]\n",
    "             [1  x₆₁  x₆₂  x₆₃]\n",
    "             [1  x₇₁  x₇₂  x₇₃]\n",
    "```\n",
    "\n",
    "**Result:**\n",
    "```py \n",
    "         Z = [z₁]   [w₀ + w₁x₁₁ + w₂x₁₂ + w₃x₁₃]\n",
    "             [z₂]   [w₀ + w₁x₂₁ + w₂x₂₂ + w₃x₂₃]\n",
    "             [z₃] = [w₀ + w₁x₃₁ + w₂x₃₂ + w₃x₃₃]\n",
    "             [z₄]   [w₀ + w₁x₄₁ + w₂x₄₂ + w₃x₄₃]\n",
    "             [z₅]   [w₀ + w₁x₅₁ + w₂x₅₂ + w₃x₅₃]\n",
    "             [z₆]   [w₀ + w₁x₆₁ + w₂x₆₂ + w₃x₆₃]\n",
    "             [z₇]   [w₀ + w₁x₇₁ + w₂x₇₂ + w₃x₇₃]\n",
    "```\n",
    "\n",
    "Where:\n",
    "- $z₁ = w₀ + w₁x₁₁ + w₂x₁₂ + w₃x₁₃$\n",
    "- $z₂ = w₀ + w₁x₂₁ + w₂x₂₂ + w₃x₂₃$\n",
    "- $z₃ = w₀ + w₁x₃₁ + w₂x₃₂ + w₃x₃₃$\n",
    "- $z₄ = w₀ + w₁x₄₁ + w₂x₄₂ + w₃x₄₃$\n",
    "- $z₅ = w₀ + w₁x₅₁ + w₂x₅₂ + w₃x₅₃$\n",
    "- $z₆ = w₀ + w₁x₆₁ + w₂x₆₂ + w₃x₆₃$\n",
    "- $z₇ = w₀ + w₁x₇₁ + w₂x₇₂ + w₃x₇₃$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244c44a8",
   "metadata": {},
   "source": [
    "**`Step 4`: Apply $ReLU$ Activation (Element-wise):**\n",
    "\n",
    "**`ReLU` Function:** $ReLU(z) = max(0, z)$\n",
    "\n",
    "```py \n",
    "         Y_pred = ReLU(Z) = [ReLU(z₁)]   [max(0, z₁)]\n",
    "                           [ReLU(z₂)]    [max(0, z₂)]\n",
    "                           [ReLU(z₃)] =  [max(0, z₃)]\n",
    "                           [ReLU(z₄)]    [max(0, z₄)]\n",
    "                           [ReLU(z₅)]    [max(0, z₅)]\n",
    "                           [ReLU(z₆)]    [max(0, z₆)]\n",
    "                           [ReLU(z₇)]    [max(0, z₇)]\n",
    "```\n",
    "\n",
    "**Predictions:**\n",
    "```py \n",
    "                   Y_pred = [ŷ₁]\n",
    "                            [ŷ₂]\n",
    "                            [ŷ₃]\n",
    "                            [ŷ₄]\n",
    "                            [ŷ₅]\n",
    "                            [ŷ₆]\n",
    "                            [ŷ₇]\n",
    "```\n",
    "**Shape**: $(7, 1)$\n",
    "\n",
    "Where:\n",
    "   - $ŷ₁ = max(0, z₁)$\n",
    "   - $ŷ₂ = max(0, z₂)$\n",
    "   - $ŷ₃ = max(0, z₃)$\n",
    "   - $ŷ₄ = max(0, z₄)$\n",
    "   - $ŷ₅ = max(0, z₅)$\n",
    "   - $ŷ₆ = max(0, z₆)$\n",
    "   - $ŷ₇ = max(0, z₇)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9075ac",
   "metadata": {},
   "source": [
    "### **Loss Calculation:**\n",
    "\n",
    "**`Step 5`: Calculate Squared Error for Each Sample:**\n",
    "\n",
    "**Squared Error Formula:** $SE = (y_{true} - y_{pred})²$\n",
    "\n",
    "For each sample:\n",
    "```py \n",
    "         SE₁ = (y₁ - ŷ₁)²\n",
    "         SE₂ = (y₂ - ŷ₂)²\n",
    "         SE₃ = (y₃ - ŷ₃)²\n",
    "         SE₄ = (y₄ - ŷ₄)²\n",
    "         SE₅ = (y₅ - ŷ₅)²\n",
    "         SE₆ = (y₆ - ŷ₆)²\n",
    "         SE₇ = (y₇ - ŷ₇)²\n",
    "```\n",
    "\n",
    "**In Vector Form:**\n",
    "```py \n",
    "         Errors = Y_true - Y_pred = [y₁ - ŷ₁]   [e₁]\n",
    "                                    [y₂ - ŷ₂]   [e₂]\n",
    "                                    [y₃ - ŷ₃] = [e₃]\n",
    "                                    [y₄ - ŷ₄]   [e₄]\n",
    "                                    [y₅ - ŷ₅]   [e₅]\n",
    "                                    [y₆ - ŷ₆]   [e₆]\n",
    "                                    [y₇ - ŷ₇]   [e₇]\n",
    "```\n",
    "\n",
    "```py \n",
    "         Squared_Errors = [e₁²]   [(y₁ - ŷ₁)²]\n",
    "                          [e₂²]   [(y₂ - ŷ₂)²]\n",
    "                          [e₃²] = [(y₃ - ŷ₃)²]\n",
    "                          [e₄²]   [(y₄ - ŷ₄)²]\n",
    "                          [e₅²]   [(y₅ - ŷ₅)²]\n",
    "                          [e₆²]   [(y₆ - ŷ₆)²]\n",
    "                          [e₇²]   [(y₇ - ŷ₇)²]\n",
    "```\n",
    "\n",
    "**`Step 6`: Calculate Total Loss (Sum of Squared Errors):**\n",
    "\n",
    "**Total Loss:**\n",
    "```py \n",
    "      Loss_total = SE₁ + SE₂ + SE₃ + SE₄ + SE₅ + SE₆ + SE₇\n",
    "\n",
    "      Loss_total = (y₁ - ŷ₁)² + (y₂ - ŷ₂)² + (y₃ - ŷ₃)² + (y₄ - ŷ₄)² \n",
    "                  + (y₅ - ŷ₅)² + (y₆ - ŷ₆)² + (y₇ - ŷ₇)²\n",
    "```\n",
    "\n",
    "**Compact Notation:**\n",
    "```py \n",
    "         Loss_total = Σᵢ₌₁⁷ (yᵢ - ŷᵢ)²\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07c1562e",
   "metadata": {},
   "source": [
    "**Step 7: Calculate Loss Per Epoch (Mean Squared Error):**\n",
    "\n",
    "**Loss Per Epoch (Average Loss):**\n",
    "```py \n",
    "      Loss_per_epoch = Loss_total / batch_size\n",
    "\n",
    "      Loss_per_epoch = [(y₁ - ŷ₁)² + (y₂ - ŷ₂)² + (y₃ - ŷ₃)² + (y₄ - ŷ₄)² \n",
    "                     + (y₅ - ŷ₅)² + (y₆ - ŷ₆)² + (y₇ - ŷ₇)²] / 7\n",
    "```\n",
    "\n",
    "**Compact Notation:**\n",
    "\n",
    "> $$\\text{Loss}_{\\text{per epoch}} = \\frac{1}{7} \\sum_{i=1}^{7} (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "This is also known as **Mean Squared Error ($MSE$)**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028201f8",
   "metadata": {},
   "source": [
    "| Step | Operation | Input Shape | Output Shape | Result |\n",
    "|------|-----------|-------------|--------------|--------|\n",
    "| 1 | Original Input | $(7, 3)$ | $(7, 3)$ | $X$ |\n",
    "| 2 | Bias Augmentation | $(7, 3)$ | $(7, 4)$ | $X_{aug}$ |\n",
    "| 3 | Linear Transform | $(7, 4) × (4, 1)$ | $(7, 1)$ | $Z$ |\n",
    "| 4 | ReLU Activation | $(7, 1)$ | $(7, 1)$ | $Y_{pred}$ |\n",
    "| 5 | Error Calculation | $(7, 1) - (7, 1)$ | $(7, 1)$ | Errors |\n",
    "| 6 | Squared Errors | $(7, 1)$ | $(7, 1)$ | $SE$ |\n",
    "| 7 | Total Loss | sum of $(7, 1)$ | scalar | $Loss\\_total$ |\n",
    "| 8 | Loss Per Epoch | Loss_total / 7 | scalar | $MSE$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e41c4ac0",
   "metadata": {},
   "source": [
    "## **Key Concepts:**\n",
    "\n",
    "1. **`Batched Processing`**: All 7 samples are processed simultaneously in one matrix operation, which is much more efficient than processing one at a time.\n",
    "\n",
    "2. **`Matrix Dimensions`**:\n",
    "   - **`Augmented Input`**: $(7, 4)$ represents 7 samples with 4 features (including bias)\n",
    "   - **`Weight Transpose`**: (4, 1) represents 4 weights for 1 output\n",
    "   - **`Output`**: $(7, 1)$ represents 7 predictions\n",
    "\n",
    "3. **`ReLU Activation`**: \n",
    "   - Outputs the input if positive, otherwise outputs 0\n",
    "   - Applied element-wise to each prediction\n",
    "\n",
    "4. **`Loss Metrics`**:\n",
    "   - **Total Loss**: Sum of all squared errors across the batch\n",
    "   - **Loss Per Epoch ($MSE$)**: Average squared error, useful for comparing across different batch sizes\n",
    "\n",
    "5. **`Vectorization`**: Using matrix operations allows the entire batch to be processed in parallel, which is the foundation of efficient deep learning.\n",
    "\n",
    "6. **`One Forward Pass = One Epoch`**: Since our batch size equals the dataset size, processing the batch once completes one full epoch."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
