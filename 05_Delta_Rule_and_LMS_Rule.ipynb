{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9436e41c",
   "metadata": {},
   "source": [
    "# **`Delta Rule` or `LMS Rule` or `Widrow-Hoff Rule`:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb3c81",
   "metadata": {},
   "source": [
    "> **Delta Rule:** __https://en.wikipedia.org/wiki/Delta_rule__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caaad84c",
   "metadata": {},
   "source": [
    "> ![](https://media.geeksforgeeks.org/wp-content/uploads/20240515182018/Widrow_Hoff-Algorithm-.webp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b453aa",
   "metadata": {},
   "source": [
    "Delta Rule is the **`missing bridge`** between biologically inspired learning (*Hebb*, *Oja*) and **`task-driven learning`** (*Perceptron*, *modern neural networks*). \n",
    "\n",
    "## **What is the Delta Rule? (high-level):**  \n",
    "\n",
    "The **`Delta Rule`** (also called the **`LMS rule`** or **`Widrow–Hoff rule`**) is a **`supervised learning rule`** that updates weights to **reduce the difference (`delta`)** between a neuron’s output and a desired target.\n",
    "\n",
    "**In one sentence:**   \n",
    "> **The Delta Rule changes weights in proportion to how much the neuron is wrong.**\n",
    "\n",
    "This is the **first time `“error”` explicitly enters learning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce284fdb",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082bc54a",
   "metadata": {},
   "source": [
    "## **Why the Delta Rule was Necessary (the gap before it):**\n",
    "\n",
    "Up to Oja’s rule, learning had these properties:   \n",
    "   - ✔ Local\n",
    "   - ✔ Biologically motivated\n",
    "   - ✔ Stable\n",
    "   - ✔ Unsupervised\n",
    "\n",
    "But it lacked one crucial thing:   \n",
    "   - ❌ **No notion of correctness**\n",
    "\n",
    "**Hebb/Oja ask:**   \n",
    "> “What patterns are frequent?”\n",
    "\n",
    "The Delta Rule asks:   \n",
    "> **“Was the output right or wrong?”**\n",
    "\n",
    "This shift is *`fundamental`*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dd1bea",
   "metadata": {},
   "source": [
    "### **How the Delta Rule connects to previous models:**\n",
    "\n",
    "| Model          | What it contributes              |\n",
    "| -------------- | -------------------------------- |\n",
    "| $MCP$ neuron     | Computation (threshold unit)     |\n",
    "| $Hebb$           | Correlation-based learning       |\n",
    "| $Oja$            | Stabilized unsupervised learning |\n",
    "| **`Delta Rule`** | Error-driven supervised learning |\n",
    "| $Perceptron$     | Classification with thresholds   |\n",
    "\n",
    "So the Delta Rule **`does not replace Hebb/Oja`** — it **`extends learning with feedback`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251073a8",
   "metadata": {},
   "source": [
    "## **Deriving the Delta Rule from first principles:**\n",
    "\n",
    "**Step 1: Define the neuron model (linear neuron):**\n",
    "\n",
    "We begin with a **linear neuron** (important):\n",
    "\n",
    "> $y = w^\\top x$ \n",
    "\n",
    "No step function yet — we need differentiability.\n",
    "\n",
    "**Step 2: Introduce a learning objective:**\n",
    "\n",
    "We want the output (y) to match a target (t).\n",
    "\n",
    "Define **error**:\n",
    "\n",
    "> $e = t - y$ \n",
    "\n",
    "Learning should **reduce this error**.\n",
    "\n",
    "**Step 3: Define a loss function:**\n",
    "\n",
    "The simplest smooth loss is **squared error**:   \n",
    "> $E = \\frac{1}{2}(t - y)^2$ \n",
    "\n",
    "**Why squared?**  \n",
    "   * Always positive\n",
    "   * Penalizes large errors\n",
    "   * Differentiable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1551e000",
   "metadata": {},
   "source": [
    "**Step 4: Minimize error using gradient descent:**\n",
    "\n",
    "We update weights in the direction that **reduces error fastest**:\n",
    "\n",
    "> $\\Delta w = -\\eta \\frac{\\partial E}{\\partial w}$ \n",
    "\n",
    "**Step 5: Compute the gradient:**\n",
    "\n",
    "> $\\frac{\\partial E}{\\partial w}\n",
    "= \\frac{\\partial E}{\\partial y} \\cdot \\frac{\\partial y}{\\partial w}$ \n",
    "\n",
    "> $= -(t - y) x$ \n",
    "\n",
    "**Step 6: Final Delta Rule:**\n",
    "\n",
    "Substitute into gradient descent:\n",
    "\n",
    "> $\\boxed{\n",
    "\\Delta w = \\eta (t - y) x\n",
    "}$\n",
    "\n",
    "This is the **`Delta Rule`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8409241d",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c56ece5",
   "metadata": {},
   "source": [
    "## **Intuitional meaning (very important):**\n",
    "\n",
    "The Delta Rule says:   \n",
    "   * If output is **too small** → increase weights\n",
    "   * If output is **too large** → decrease weights\n",
    "   * Change is proportional to:\n",
    "     * Input strength\n",
    "     * Error magnitude\n",
    "\n",
    "**In words:**   \n",
    "> `“Change weights only when you are wrong, and change them more when you are very wrong.”`\n",
    "\n",
    "This is **`learning from mistakes`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342ec9ae",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13d320e9",
   "metadata": {},
   "source": [
    "## **Relationship to Hebbian learning:**\n",
    "\n",
    "Recall Hebb:\n",
    "\n",
    "> $\\Delta w = \\eta x y$ \n",
    "\n",
    "Delta Rule:\n",
    "\n",
    "> $\\Delta w = \\eta x (t - y)$\n",
    "\n",
    "Notice:\n",
    "\n",
    "* Hebb uses **output activity**\n",
    "* Delta rule uses **error signal**\n",
    "\n",
    "You can think of Delta Rule as:\n",
    "\n",
    "> **Hebbian learning modulated by error**\n",
    "\n",
    "This is a major conceptual leap."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6f313f",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9009607b",
   "metadata": {},
   "source": [
    "## **Geometric meaning of the Delta Rule:**\n",
    "\n",
    "This is where the rule becomes visually clear.\n",
    "\n",
    "**1. Weight space interpretation:**    \n",
    "   * Each weight vector defines a **hyperplane**\n",
    "   * Error defines a **direction of correction**\n",
    "   * Gradient descent moves the hyperplane to reduce misalignment\n",
    "\n",
    "**2. Data space interpretation:**     \n",
    "   * The neuron projects data onto ($w$)\n",
    "   * Delta rule rotates and shifts ($w$)\n",
    "   * Until predictions match targets in least-squares sense\n",
    "\n",
    "**Key result:**   \n",
    "> The Delta Rule finds the **best linear approximation** to the target function.\n",
    "\n",
    "This is linear regression in disguise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49988dc2",
   "metadata": {},
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a242cef",
   "metadata": {},
   "source": [
    "## **What the Delta Rule successfully achieves?**\n",
    "\n",
    "By this stage, we have:\n",
    "\n",
    "✅ Error-driven learning   \n",
    "✅ Supervised learning     \n",
    "✅ Stable convergence            \n",
    "✅ Clear mathematical objective   \n",
    "✅ Geometric interpretation       \n",
    "✅ Foundation of gradient descent    \n",
    "\n",
    "This is **`huge`** — it introduces optimization to learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb8ee88",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d477e4e6",
   "metadata": {},
   "source": [
    "## **What Still Remains (why this isn’t the final perceptron)?**\n",
    "\n",
    "Even now, limitations remain:\n",
    "\n",
    "❌ No nonlinearity (yet)                         \n",
    "❌ No classification boundary enforcement        \n",
    "❌ Linear outputs only                        \n",
    "❌ Cannot solve non-linear problems          \n",
    "❌ No discrete decisions                         \n",
    "\n",
    "To fix this, we need:    \n",
    "> **A thresholded output + classification-specific updates**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edc6c6f",
   "metadata": {},
   "source": [
    "--------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c848f6da",
   "metadata": {},
   "source": [
    "## **What Comes Next in the Pipeline?**\n",
    "\n",
    "Here is the **logical continuation**:\n",
    "\n",
    "1. **Perceptron Learning Rule**  \n",
    "   * Adds step activation\n",
    "   * Updates only on misclassification\n",
    "\n",
    "2. **Multi-layer Perceptron ($MLP$)**  \n",
    "   * Hidden layers\n",
    "   * Nonlinear representations\n",
    "\n",
    "3. **Backpropagation**  \n",
    "   * Error propagation through layers\n",
    "\n",
    "4. **Modern Deep Learning**  \n",
    "   * Advanced activations\n",
    "   * Optimizers ($Adam$, $RMSProp$)\n",
    "   * Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b18dffa",
   "metadata": {},
   "source": [
    "> **The Delta Rule introduces supervised, error-driven learning by minimizing squared error via gradient descent, forming the mathematical foundation of the perceptron and all modern neural network training.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
