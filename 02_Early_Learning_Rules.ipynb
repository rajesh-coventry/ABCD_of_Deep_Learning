{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db9c55a9",
   "metadata": {},
   "source": [
    "# **Early Learning Rules: The History:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4376997",
   "metadata": {},
   "source": [
    "## **Hebbian Learning:**\n",
    "\n",
    "> ![](https://ars.els-cdn.com/content/image/1-s2.0-S0149763419310942-gr9.jpg) \n",
    "\n",
    "Hebbian Learning is a foundational theory of biological learning and synaptic plasticity introduced by Canadian psychologist Donald O. Hebb in his 1949 book, `The Organization of Behavior`. The concept is best summarized by the famous principle: *`\"Cells that fire together, wire together.\"`*\n",
    "\n",
    "> *The concept states: When an axon of cell $A$ is near enough to excite cell $B$ and repeatedly or persistently takes part in firing it, some growth process or metabolic change takes place in one or both cells such that $A$'s efficiency in firing $B$ is increased.*\n",
    "\n",
    "**Hebbian learning** is a principle that describes how the strength of a connection between two neurons changes based on their activity. In its simplest form, it states:\n",
    "\n",
    "> *When one neuron repeatedly contributes to the firing of another neuron, the connection between them becomes stronger.*\n",
    "\n",
    "This principle means that when two neurons are simultaneously and repeatedly active (or `\"co-activated\"`), the strength of the synaptic connection (the biological `\"weight\"`) between them gets physically or functionally stronger. This reinforcement creates a stable, lasting association, which is the biological mechanism underlying memory formation and learning of patterns and habits\n",
    "\n",
    "At a conceptual level, Hebbian learning says that **`correlation implies learning`**:      \n",
    "   * If a presynaptic neuron is active\n",
    "   * And a postsynaptic neuron is active at the same time\n",
    "   * Then the synapse connecting them should be strengthened"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4566bd5",
   "metadata": {},
   "source": [
    "In the context of Artificial Neural Networks, Hebbian learning is often mathematically described by a simple local update rule:\n",
    "\n",
    "> $$\\Delta w_{ij} = \\eta \\cdot x_i \\cdot y_j$$\n",
    "\n",
    "Where:\n",
    "   * $\\Delta w_{ij}$ is the change in the weight connecting pre-synaptic neuron $i$ to post-synaptic neuron $j$.\n",
    "   * $\\eta$ (eta) is the learning rate (a constant controlling the step size).\n",
    "   * $x_i$ is the activity (input/activation) of the pre-synaptic neuron $i$.\n",
    "   * $y_j$ is the activity (output/activation) of the post-synaptic neuron $j$.\n",
    "\n",
    "This rule is **`local`** (only depends on the two neurons involved), **`unsupervised`** (no teacher or label), and **`activity-driven`**.\n",
    "\n",
    "The key takeaway is that the weight change is **`proportional to the product of the two neuron activities`**. If both $x_i$ and $y_j$ are positive (active), the weight $w_{ij}$ increases, reinforcing the connection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9ec281",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4cb94",
   "metadata": {},
   "source": [
    "### **Hebbian Learning, First Proposed in Biology:**\n",
    "\n",
    "> **Hebbian learning was first proposed in neuroscience, not in machine learning**.\n",
    "\n",
    "**Historical origin:**   \n",
    "   * Proposed by **Donald O. Hebb** in **1949**\n",
    "   * In his book: *“The Organization of Behavior”*\n",
    "   * Hebb was a psychologist/neuroscientist, not a computer scientist\n",
    "\n",
    "**Importantly**:  \n",
    "   * Hebb did **not** propose a detailed biological mechanism\n",
    "   * He proposed a **theoretical principle** to explain learning and memory in the brain\n",
    "\n",
    "**Hebb was trying to answer:**   \n",
    "> **How can experience change the brain in a lasting way?**\n",
    "\n",
    "**His answer was:**    \n",
    "> **Changes in synaptic strength caused by correlated activity form the basis of learning and memory.**\n",
    "\n",
    "Decades later, experimental neuroscience found biological phenomena (like LTP) that strongly supported Hebb’s hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81dae7a3",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0404d613",
   "metadata": {},
   "source": [
    "## **How was Hebbian Learning Adapted into Artificial Neural Networks?**\n",
    "\n",
    "Hebbian learning was **not originally designed for artificial networks**, but it became **one of the earliest inspirations** for them.\n",
    "\n",
    "**Hebb’s rule had several appealing properties:**   \n",
    "   * Simple\n",
    "   * Local (no global control needed)\n",
    "   * Biologically motivated\n",
    "   * Easy to express mathematically\n",
    "\n",
    "**Early researchers thought:**    \n",
    "> “If the brain learns by strengthening connections, maybe machines can learn the same way.”\n",
    "\n",
    "Thus, Hebbian learning became a **`conceptual bridge`** between biological neurons and artificial ones.\n",
    "\n",
    "**However, in artificial systems:**   \n",
    "   * Neural activity became numerical values\n",
    "   * Synaptic strength became adjustable weights\n",
    "   * Biological spikes became continuous signals or binary outputs\n",
    "\n",
    "This transformation turned Hebb’s biological idea into a **`mathematical learning rule`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be95bd7",
   "metadata": {},
   "source": [
    "While modern Deep Learning (especially the training via `backpropagation` and `Stochastic Gradient Descent`) is technically non-Hebbian and non-local (it requires an error signal from the far end of the network), the intuitive principle of Hebbian learning remains a core motivator and is used in specific models:\n",
    "   - **`Unsupervised Learning`:** Hebbian rules (or their variants like `Oja's rule` and `Spike-Timing-Dependent Plasticity` ($STDP$)) are perfectly suited for unsupervised learning. They allow neurons to automatically discover patterns and extract relevant features (like edges in images) simply by noticing which inputs co-occur frequently.\n",
    "\n",
    "   - **`Associative Memory`:** Hebbian learning is the basis for models like the `Hopfield Network`, which acts as a type of content-addressable memory, where the network learns associations between patterns, allowing the recall of a complete memory from a partial cue. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c810045",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed0cb0d",
   "metadata": {},
   "source": [
    "## **Is Hebbian Learning the First Attempt to Approximate a Human Neuron Mathematically?**\n",
    "\n",
    "> **No — but it is one of the earliest and most influential attempts.**\n",
    "\n",
    "**Before Hebb:**\n",
    "\n",
    "1. **McCulloch & Pitts (1943)** proposed the first mathematical neuron    \n",
    "  * Binary inputs\n",
    "  * Weighted sum\n",
    "  * Threshold activation\n",
    "  * No learning (weights were fixed)\n",
    "\n",
    "2. **Hebb (1949):**   \n",
    "   * Introduced a **learning principle**\n",
    "   * Explained how synapses might change over time\n",
    "   * Did not define a neuron’s computation, but defined how it *adapts*\n",
    "\n",
    "**Rosenblatt’s Perceptron (1957):**   \n",
    "* Combined:   \n",
    "   * McCulloch–Pitts neuron (computation)\n",
    "   * Hebbian-style ideas (learning)\n",
    "* Added supervised error correction\n",
    "\n",
    "**So historically:**\n",
    "\n",
    "1. **`McCulloch–Pitts`** → neuron model (no learning)\n",
    "2. **`Hebb`** → learning principle (no formal neuron)\n",
    "3. **`Perceptron`** → neuron + learning rule\n",
    "\n",
    "Hebbian learning was **`the first serious attempt to explain learning at the synaptic level`**, not the first neuron model itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2360dba6",
   "metadata": {},
   "source": [
    "---------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "779df573",
   "metadata": {},
   "source": [
    "## **What Hebbian Learning Gets Right (Conceptually):**\n",
    "\n",
    "**Hebbian learning correctly captures several deep biological truths:**  \n",
    "   * Learning is **`local`** to synapses\n",
    "   * Correlation matters\n",
    "   * Experience changes connectivity\n",
    "   * Memory can be stored in connection strengths\n",
    "   * Learning does not require an explicit teacher\n",
    "\n",
    "These ideas remain foundational in neuroscience and influence modern $AI$ thinking."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d10445b",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dfa107",
   "metadata": {},
   "source": [
    "## **What Hebbian Learning gets Wrong or Oversimplifies:**\n",
    "\n",
    "Despite its importance, Hebbian learning is **`not sufficient`** to explain real brain learning:         \n",
    "   * It causes **`runaway weight growth`** without normalization\n",
    "   * It ignores spike timing (later addressed by STDP)\n",
    "   * It cannot explain goal-directed learning by itself\n",
    "   * It lacks stability mechanisms\n",
    "   * It does not scale to complex cognition\n",
    "\n",
    "That’s why modern deep learning relies on **`error-driven learning (backpropagation)`** rather than pure Hebbian rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52705d26",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379ea2d",
   "metadata": {},
   "source": [
    "## **Why Hebbian Learning is Still Taught Today?**\n",
    "\n",
    "**Hebbian learning is taught because it:**   \n",
    "   * Explains *`why`* weights exist at all\n",
    "   * Builds intuition for learning\n",
    "   * Provides historical grounding\n",
    "   * Shows the biological roots of neural networks\n",
    "   * Introduces the idea of plasticity\n",
    "\n",
    "It is a **`conceptual foundation`**, not a practical training method for deep networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f531f91",
   "metadata": {},
   "source": [
    "> **Hebbian learning is a biologically inspired principle, first proposed to explain learning in the brain, that states synaptic connections strengthen through correlated activity; it was not the first mathematical neuron, but it was the first influential theory of learning that bridged biology and artificial neural models.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef04127",
   "metadata": {},
   "source": [
    "-----------\n",
    "-----\n",
    "-------\n",
    "--------\n",
    "------"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
