{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4e3fa13",
   "metadata": {},
   "source": [
    "# **Equation of a line:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbefb8a",
   "metadata": {},
   "source": [
    "> ![](https://cdn1.byjus.com/wp-content/uploads/2021/02/Line-Segment-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018e8782",
   "metadata": {},
   "source": [
    "A line is the shortest path between two points, extended infinitely in both directions.\n",
    "\n",
    "Line has a constant **`slope`** (rate of change) everywhere.\n",
    "\n",
    "**Given two points:** $P₁ = (x₁, y₁)$ and $P₂ = (x₂, y₂)$\n",
    "\n",
    "Then, slope defines how much $y$ changes for each unit change in $x$.\n",
    "\n",
    "> **slope** = **rise/run** = $\\frac{y_2 - y_1}{x_2 - x_1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb77f434",
   "metadata": {},
   "source": [
    "> ![](https://i.ytimg.com/vi/jlkE4VCnhdE/maxresdefault.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b4ce08a",
   "metadata": {},
   "source": [
    "**Let's call this:** \n",
    "> $$m = \\frac{y_2 - y_1}{x_2 - x_1}$$\n",
    "\n",
    "**What this means:** If I move $1$ unit to the right $(Δx = 1)$, I move $m$ units up $(Δy = m)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d1032e",
   "metadata": {},
   "source": [
    "> ![](https://www.mathplanet.com/Oldsite/media/38362/slope01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb6179b",
   "metadata": {},
   "source": [
    "#### **Equation for ANY Point on the Line:**\n",
    "\n",
    "For ANY point $(x, y)$ on the line, the slope from $P₁$ to that point must equal $m$.\n",
    "\n",
    "Using point $P₁ = (x₁, y₁)$ and a general point $P = (x, y)$:\n",
    "\n",
    "> $$\\frac{y - y_1}{x - x_1} = m$$\n",
    "\n",
    "The slope from $P₁$ to any point $P$ on the line equals the line's slope. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ce545b",
   "metadata": {},
   "source": [
    "Cross-multiply:\n",
    "\n",
    "> $y - y_1 = m(x - x_1)$\n",
    "\n",
    "The change in $y$ equals the slope times the change in $x$.\n",
    "\n",
    "Expand the right side:\n",
    "\n",
    "> $y - y_1 = m x - m x_1$  \n",
    "\n",
    "Add $y_1$ to both sides:\n",
    "\n",
    "> $y = m x - m x_1 + y_1$\n",
    "\n",
    "**Rearrange:**\n",
    "\n",
    "> $y = m x + (y_1 - m x_1)$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217f0db",
   "metadata": {},
   "source": [
    "#### **Define the Intercept:** \n",
    "\n",
    "The term $(y₁ - mx₁)$ is a constant. Let's call it **$b$**:\n",
    "\n",
    "> $b = y₁ - mx₁$  \n",
    "\n",
    "**What is $b$ here?** It's the $y$-value where the line crosses the y-axis (when $x = 0$):\n",
    "- When $x = 0$: $y = m(0) + b = b$\n",
    "\n",
    "**Physical meaning:** $b$ is the `\"starting height\"` of the line when $x = 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429b127",
   "metadata": {},
   "source": [
    "> ![](https://media.geeksforgeeks.org/wp-content/uploads/20230529095322/X-and-Y-Intercepts-of-a-Line.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181ba588",
   "metadata": {},
   "source": [
    "**The Famous Form:**\n",
    "\n",
    "> $y = mx + b$ \n",
    "\n",
    "Where:\n",
    "- **$m$** = slope (how steep the line is)\n",
    "- **$b$** = y-intercept (where the line crosses the y-axis)\n",
    "\n",
    "**Alternative notation (commonly used in machine learning):**\n",
    "\n",
    "> $y = wx + b$ \n",
    "\n",
    "Where **$w$** stands for `\"weight\"` instead of $m$ for `\"slope\"` (same concept, different name)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35894b4",
   "metadata": {},
   "source": [
    "> ![](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRvTMau2WDuwaxNOVKmoptXK7VMyGpbBkIuqA&s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36879a1",
   "metadata": {},
   "source": [
    "#### **From $1D$ Input to $nD$ Input:**\n",
    "\n",
    "**Current equation:** $y = wx + b$ (one input $x$, one output $y$)\n",
    "\n",
    "What if we have multiple inputs: $x₁, x₂, x₃, ..., xₙ$?\n",
    "\n",
    "If each input contributes independently to the output then,  \n",
    "\n",
    "> $$y = w_1 x_1 + w_2 x_2 + w_3 x_3 + \\cdots + w_n x_n + b$$\n",
    "> $$y = \\sum_{i=1}^{n} w_i x_i + b$$\n",
    "\n",
    "Each input $xᵢ$ has its own weight $wᵢ$ that determines how much it contributes to $y$. Then we add the intercept $b$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f87e0b",
   "metadata": {},
   "source": [
    "-----\n",
    "----\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a36311",
   "metadata": {},
   "source": [
    "## **Equation of Line in Vector Form:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eef9ce",
   "metadata": {},
   "source": [
    "We already know the equation of a line:\n",
    "\n",
    "> $y = wx + b$ \n",
    "\n",
    "Here:\n",
    "\n",
    "* ($x$) → input value\n",
    "* ($w$) → slope (how strongly ($x$) affects ($y$))\n",
    "* ($b$) → intercept (baseline shift)\n",
    "\n",
    "This is a **`scalar equation`** (single input, single weight).\n",
    "\n",
    "Real problems rarely depend on just one variable. Suppose the output depends on **`two inputs`**:\n",
    "\n",
    "> $y = w_1 x_1 + w_2 x_2 + b$ \n",
    "\n",
    "Conceptually:   \n",
    "   * Each input contributes independently\n",
    "   * Each contribution is scaled by its own importance\n",
    "\n",
    "This is already a **`linear combination`**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7c930f",
   "metadata": {},
   "source": [
    "#### **Generalize to ($n$) inputs (explicit summation):**\n",
    "\n",
    "For ($n$) inputs:\n",
    "\n",
    "> $y = w_1 x_1 + w_2 x_2 + \\dots + w_n x_n + b$ \n",
    "\n",
    "This can be written compactly using summation notation:\n",
    "\n",
    "> $y = \\sum_{i=1}^{n} w_i x_i + b$\n",
    "\n",
    "At this point:\n",
    "\n",
    "* ($x_i$) = components of the input\n",
    "* ($w_i$) = corresponding weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48896f5f",
   "metadata": {},
   "source": [
    "#### **Introduce vectors (grouping related quantities):**\n",
    "\n",
    "Instead of treating each input separately, we **group them into vectors**.\n",
    "\n",
    "Define the **input vector**:\n",
    "\n",
    "> $\\mathbf{x} =\n",
    "\\begin{bmatrix}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "\\vdots \\\\\n",
    "x_n\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "Define the **weight vector**:\n",
    "\n",
    "> $\\mathbf{w} =\n",
    "\\begin{bmatrix}\n",
    "w_1 \\\\\n",
    "w_2 \\\\\n",
    "\\vdots \\\\\n",
    "w_n\n",
    "\\end{bmatrix}$ \n",
    "\n",
    "Conceptually:     \n",
    "   * A vector represents a **point or direction** in space\n",
    "   * Grouping allows geometric interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441e0b43",
   "metadata": {},
   "source": [
    "#### **Recognize the dot product:**\n",
    "\n",
    "The dot product of two vectors is defined as:\n",
    "\n",
    "> $\\mathbf{w}^\\top \\mathbf{x} = \\sum_{i=1}^{n} w_i x_i$ \n",
    "\n",
    "So the summation we already had **`is exactly a dot product`**.\n",
    "\n",
    "This is not a trick — it’s a definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1abfbf",
   "metadata": {},
   "source": [
    "#### **Replace the summation with dot product notation:**\n",
    "\n",
    "Substitute into the equation:\n",
    "\n",
    "> $y = \\mathbf{w}^\\top \\mathbf{x} + b$ \n",
    "\n",
    "This is the **vector form** of:\n",
    "\n",
    "> $y = wx + b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a653a6a4",
   "metadata": {},
   "source": [
    "#### **Why we write ($w^\\top x$) and not ($wx$):**\n",
    "\n",
    "Vectors by default are **`columns`**.   \n",
    "   * ($\\mathbf{x}) is (n \\times 1$)\n",
    "   * ($\\mathbf{w}) is (n \\times 1$)\n",
    "\n",
    "**To multiply them:**   \n",
    "   * ($\\mathbf{w}^\\top) becomes (1 \\times n$)\n",
    "   * ($\\mathbf{w}^\\top \\mathbf{x}$) becomes a scalar\n",
    "\n",
    "This preserves:   \n",
    "   * Dimensional correctness\n",
    "   * Mathematical consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847626f3",
   "metadata": {},
   "source": [
    "#### **Geometric meaning of ( $\\mathbf{w}^\\top \\mathbf{x}$ ):**\n",
    "\n",
    "The dot product measures:\n",
    "\n",
    "> $\\mathbf{w}^\\top \\mathbf{x} = |\\mathbf{w}| |\\mathbf{x}| \\cos \\theta$ \n",
    "\n",
    "So it captures:   \n",
    "   * Alignment between input and weight direction\n",
    "   * Strength of match\n",
    "\n",
    "In neural terms:   \n",
    "> “How well does this input match what the neuron is looking for?”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9e16e6",
   "metadata": {},
   "source": [
    "#### **Role of the bias ($b$):**\n",
    "\n",
    "**The bias:**   \n",
    "   * Shifts the output up or down\n",
    "   * Moves the decision boundary away from the origin\n",
    "\n",
    "Geometrically:   \n",
    "> $\\mathbf{w}^\\top \\mathbf{x} + b = 0$ \n",
    "\n",
    "defines a **shifted hyperplane**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce5fe25",
   "metadata": {},
   "source": [
    "---------------\n",
    "----\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30afda76",
   "metadata": {},
   "source": [
    "## **Auggmentation:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1090e060",
   "metadata": {},
   "source": [
    "We begin with the standard linear model:\n",
    "\n",
    "> $y = \\mathbf{w}^\\top \\mathbf{x} + b$ \n",
    " \n",
    "where:\n",
    "- $\\mathbf{w} = [w_1, w_2, \\dots, w_n]^\\top$ is the weight column vector\n",
    "- $\\mathbf{x} = [x_1, x_2, \\dots, x_n]^\\top$  is the input column vector\n",
    "- $b$ is the bias term (scalar)\n",
    "\n",
    "Write out the dot product explicitly:\n",
    "\n",
    "> $y = w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b$ \n",
    "\n",
    "We can treat the bias $b$ as an additional weight $w_0$ if we add a corresponding input feature with a constant value of $1$. Let's define:\n",
    "\n",
    "> $w_0 = b$ \n",
    "\n",
    "Now, instead of putting the $1$ at the end, we'll put it **at the beginning**:\n",
    "\n",
    "**Augmented Weight Vector:**\n",
    "\n",
    "> $\\tilde{\\mathbf{w}} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}$ \n",
    "\n",
    "where $w_0 = b$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3ebe86",
   "metadata": {},
   "source": [
    "**Augmented Input Vector:**\n",
    "\n",
    "> $\\tilde{\\mathbf{x}} = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ \n",
    "\n",
    "Now the equation becomes:\n",
    "\n",
    "> $y = \\tilde{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}}$ \n",
    "\n",
    "**Let's verify this:**\n",
    "\n",
    "> $\\tilde{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}} = [w_0, w_1, w_2, \\dots, w_n] \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ \n",
    "> \n",
    "> $= w_0 \\cdot 1 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n$ \n",
    "> \n",
    "> $= b \\cdot 1 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n$ \n",
    "> \n",
    "> $= w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n + b$ \n",
    "\n",
    "Which matches our original equation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea3419f",
   "metadata": {},
   "source": [
    "#### **Original Form:**\n",
    "\n",
    "> $y = \\mathbf{w}^\\top \\mathbf{x} + b$ \n",
    "> \n",
    "> $\\mathbf{w} = \\begin{bmatrix} w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}, \\quad \\mathbf{x} = \\begin{bmatrix} x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ \n",
    "\n",
    "**Augmented Form (with 1 at beginning):**\n",
    "\n",
    "> $y = \\tilde{\\mathbf{w}}^\\top \\tilde{\\mathbf{x}}$ \n",
    "\n",
    "> $\\tilde{\\mathbf{w}} = \\begin{bmatrix} w_0 \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix} = \\begin{bmatrix} b \\\\ w_1 \\\\ w_2 \\\\ \\vdots \\\\ w_n \\end{bmatrix}, \\quad \\tilde{\\mathbf{x}} = \\begin{bmatrix} 1 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{bmatrix}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac03c7c",
   "metadata": {},
   "source": [
    "#### **Key Advantages:**\n",
    "1. **`Unified representation`**: The bias is now just another weight\n",
    "\n",
    "2. **`Simplified notation`**: No separate \"+ b\" term\n",
    "\n",
    "3. **`Matrix operations`**: Easier to work with in linear algebra and optimization\n",
    "\n",
    "4. **`Consistency`**: All parameters are in one vector\n",
    "\n",
    "5. **`Gradient computation`**: Derivatives become cleaner in machine learning\n",
    "\n",
    "This augmented form is used in machine learning implementations, where we prepend a column of 1's to the data matrix $X$ to absorb the bias term into the weight vector."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0002688f",
   "metadata": {},
   "source": [
    "> **Vector notation expresses a linear function as a dot product between input and weight vectors, allowing a single equation to scale from simple lines to high-dimensional decision boundaries.**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
